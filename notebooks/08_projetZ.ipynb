{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2516, 58)\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "ebay_df = pd.read_csv('../data/merged_data/EBAY.csv')\n",
    "ebay_df = ebay_df[(ebay_df['Date'] >= start_date) & (ebay_df['Date'] <= end_date)]\n",
    "\n",
    "# Convertir la colonne 'Date' en datetime et la définir comme index\n",
    "ebay_df['Date'] = pd.to_datetime(ebay_df['Date'])\n",
    "ebay_df.set_index('Date', inplace=True)\n",
    "\n",
    "print(ebay_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>stock_reddit_neg</th>\n",
       "      <th>stock_reddit_neu</th>\n",
       "      <th>stock_reddit_pos</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.983682</td>\n",
       "      <td>0.972329</td>\n",
       "      <td>0.969038</td>\n",
       "      <td>0.973833</td>\n",
       "      <td>0.973351</td>\n",
       "      <td>0.843496</td>\n",
       "      <td>0.985551</td>\n",
       "      <td>0.983504</td>\n",
       "      <td>0.981922</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604342</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.998224</td>\n",
       "      <td>0.985285</td>\n",
       "      <td>0.979239</td>\n",
       "      <td>0.983856</td>\n",
       "      <td>0.984194</td>\n",
       "      <td>0.854856</td>\n",
       "      <td>0.996702</td>\n",
       "      <td>0.994839</td>\n",
       "      <td>0.993329</td>\n",
       "      <td>0.989319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311568</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019667</td>\n",
       "      <td>0.938121</td>\n",
       "      <td>0.042211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1.007277</td>\n",
       "      <td>0.993958</td>\n",
       "      <td>0.986298</td>\n",
       "      <td>0.990085</td>\n",
       "      <td>0.991047</td>\n",
       "      <td>0.862534</td>\n",
       "      <td>1.002507</td>\n",
       "      <td>1.001006</td>\n",
       "      <td>0.999713</td>\n",
       "      <td>0.995957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016661</td>\n",
       "      <td>0.867867</td>\n",
       "      <td>0.115472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1.020319</td>\n",
       "      <td>1.007777</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>1.001507</td>\n",
       "      <td>1.003250</td>\n",
       "      <td>0.874843</td>\n",
       "      <td>1.011585</td>\n",
       "      <td>1.010696</td>\n",
       "      <td>1.009822</td>\n",
       "      <td>1.006975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>0.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.879734</td>\n",
       "      <td>0.095659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>1.006210</td>\n",
       "      <td>0.997533</td>\n",
       "      <td>0.988941</td>\n",
       "      <td>0.989885</td>\n",
       "      <td>0.992437</td>\n",
       "      <td>0.866723</td>\n",
       "      <td>0.999621</td>\n",
       "      <td>0.998865</td>\n",
       "      <td>0.998089</td>\n",
       "      <td>0.995354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288173</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.059265</td>\n",
       "      <td>0.906462</td>\n",
       "      <td>0.034273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-04      0.983682      0.972329      0.969038      0.973833   \n",
       "2010-01-05      0.998224      0.985285      0.979239      0.983856   \n",
       "2010-01-06      1.007277      0.993958      0.986298      0.990085   \n",
       "2010-01-07      1.020319      1.007777      0.999161      1.001507   \n",
       "2010-01-08      1.006210      0.997533      0.988941      0.989885   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-04       0.973351       0.843496      0.985551      0.983504   \n",
       "2010-01-05       0.984194       0.854856      0.996702      0.994839   \n",
       "2010-01-06       0.991047       0.862534      1.002507      1.001006   \n",
       "2010-01-07       1.003250       0.874843      1.011585      1.010696   \n",
       "2010-01-08       0.992437       0.866723      0.999621      0.998865   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  sp500_return_pct  \\\n",
       "Date                                    ...                     \n",
       "2010-01-04      0.981922      0.978125  ...          1.604342   \n",
       "2010-01-05      0.993329      0.989319  ...          0.311568   \n",
       "2010-01-06      0.999713      0.995957  ...          0.054552   \n",
       "2010-01-07      1.009822      1.006975  ...          0.400120   \n",
       "2010-01-08      0.998089      0.995354  ...          0.288173   \n",
       "\n",
       "            gold_return_pct  vix_close  bond_yields_close  stock_reddit_neg  \\\n",
       "Date                                                                          \n",
       "2010-01-04         2.054419  20.040001              0.055               NaN   \n",
       "2010-01-05         0.035790  19.350000              0.060               NaN   \n",
       "2010-01-06         1.591991  19.160000              0.045               NaN   \n",
       "2010-01-07        -0.246505  19.059999              0.045               NaN   \n",
       "2010-01-08         0.450091  18.129999              0.040               NaN   \n",
       "\n",
       "            stock_reddit_neu  stock_reddit_pos  sector_reddit_neg  \\\n",
       "Date                                                                \n",
       "2010-01-04               NaN               NaN                NaN   \n",
       "2010-01-05               NaN               NaN           0.019667   \n",
       "2010-01-06               NaN               NaN           0.016661   \n",
       "2010-01-07               NaN               NaN           0.024607   \n",
       "2010-01-08               NaN               NaN           0.059265   \n",
       "\n",
       "            sector_reddit_neu  sector_reddit_pos  \n",
       "Date                                              \n",
       "2010-01-04                NaN                NaN  \n",
       "2010-01-05           0.938121           0.042211  \n",
       "2010-01-06           0.867867           0.115472  \n",
       "2010-01-07           0.879734           0.095659  \n",
       "2010-01-08           0.906462           0.034273  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les premières lignes\n",
    "ebay_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\n",
    "    'stock_SMA_10', 'stock_SMA_15', 'stock_SMA_20', 'stock_SMA_50',\n",
    "    'stock_SMA_100', 'stock_SMA_200', 'stock_EMA_10', 'stock_EMA_12',\n",
    "    'stock_EMA_14', 'stock_EMA_26', 'stock_EMA_30', 'stock_EMA_50',\n",
    "    'stock_EMA_100', 'stock_ADX_14', 'stock_ADX_14_neg', 'stock_ADX_14_pos',\n",
    "    'stock_ADX_20', 'stock_ADX_20_neg', 'stock_ADX_20_pos', 'stock_ADX_25',\n",
    "    'stock_ADX_25_neg', 'stock_ADX_25_pos', 'stock_ADX_30',\n",
    "    'stock_ADX_30_neg', 'stock_ADX_30_pos', 'stock_ATR_14', 'stock_ATR_20',\n",
    "    'stock_ATR_28', 'stock_RSI_7', 'stock_RSI_14', 'stock_RSI_21',\n",
    "    'stock_Stoch_14', 'stock_Stoch_14_signal', 'stock_Stoch_21',\n",
    "    'stock_Stoch_21_signal', 'stock_Stoch_28', 'stock_Stoch_28_signal',\n",
    "    'stock_CMF_14', 'stock_CMF_20', 'stock_CMF_28', 'stock_VROC_7',\n",
    "    'stock_VROC_14', 'stock_VROC_21', 'stock_VROC_28'\n",
    "]\n",
    "\n",
    "print(len(INDICATORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>stock_Stoch_21_signal</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_Stoch_28_signal</th>\n",
       "      <th>stock_CMF_14</th>\n",
       "      <th>stock_CMF_20</th>\n",
       "      <th>stock_CMF_28</th>\n",
       "      <th>stock_VROC_7</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>stock_VROC_21</th>\n",
       "      <th>stock_VROC_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.998330</td>\n",
       "      <td>0.997355</td>\n",
       "      <td>0.996371</td>\n",
       "      <td>0.990446</td>\n",
       "      <td>0.979683</td>\n",
       "      <td>0.954701</td>\n",
       "      <td>0.998311</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.997535</td>\n",
       "      <td>0.995164</td>\n",
       "      <td>...</td>\n",
       "      <td>54.455065</td>\n",
       "      <td>55.069771</td>\n",
       "      <td>55.063123</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.015653</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>0.169889</td>\n",
       "      <td>0.176499</td>\n",
       "      <td>0.170427</td>\n",
       "      <td>0.162093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.028107</td>\n",
       "      <td>0.034721</td>\n",
       "      <td>0.039930</td>\n",
       "      <td>0.060691</td>\n",
       "      <td>0.079075</td>\n",
       "      <td>0.101644</td>\n",
       "      <td>0.023539</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.028279</td>\n",
       "      <td>0.038447</td>\n",
       "      <td>...</td>\n",
       "      <td>27.815368</td>\n",
       "      <td>29.149687</td>\n",
       "      <td>27.936042</td>\n",
       "      <td>0.168749</td>\n",
       "      <td>0.142715</td>\n",
       "      <td>0.122835</td>\n",
       "      <td>0.759882</td>\n",
       "      <td>0.810442</td>\n",
       "      <td>0.823620</td>\n",
       "      <td>0.803365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.875543</td>\n",
       "      <td>0.848016</td>\n",
       "      <td>0.825517</td>\n",
       "      <td>0.789064</td>\n",
       "      <td>0.758487</td>\n",
       "      <td>0.735855</td>\n",
       "      <td>0.886369</td>\n",
       "      <td>0.877253</td>\n",
       "      <td>0.869803</td>\n",
       "      <td>0.841344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.432497</td>\n",
       "      <td>-0.414617</td>\n",
       "      <td>-0.358564</td>\n",
       "      <td>-0.301133</td>\n",
       "      <td>-0.884136</td>\n",
       "      <td>-0.892026</td>\n",
       "      <td>-0.905211</td>\n",
       "      <td>-0.929906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.982204</td>\n",
       "      <td>0.976379</td>\n",
       "      <td>0.971752</td>\n",
       "      <td>0.950291</td>\n",
       "      <td>0.926987</td>\n",
       "      <td>0.881352</td>\n",
       "      <td>0.984685</td>\n",
       "      <td>0.982559</td>\n",
       "      <td>0.980652</td>\n",
       "      <td>0.970637</td>\n",
       "      <td>...</td>\n",
       "      <td>28.601885</td>\n",
       "      <td>29.580704</td>\n",
       "      <td>29.953366</td>\n",
       "      <td>-0.109803</td>\n",
       "      <td>-0.086347</td>\n",
       "      <td>-0.071768</td>\n",
       "      <td>-0.286930</td>\n",
       "      <td>-0.293252</td>\n",
       "      <td>-0.301400</td>\n",
       "      <td>-0.310590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.997701</td>\n",
       "      <td>0.995857</td>\n",
       "      <td>0.994971</td>\n",
       "      <td>0.986995</td>\n",
       "      <td>0.974474</td>\n",
       "      <td>0.941972</td>\n",
       "      <td>0.997422</td>\n",
       "      <td>0.996957</td>\n",
       "      <td>0.996345</td>\n",
       "      <td>0.994610</td>\n",
       "      <td>...</td>\n",
       "      <td>56.681239</td>\n",
       "      <td>56.527895</td>\n",
       "      <td>57.350407</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>-0.006633</td>\n",
       "      <td>-0.022291</td>\n",
       "      <td>-0.012426</td>\n",
       "      <td>-0.031853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.013807</td>\n",
       "      <td>1.016704</td>\n",
       "      <td>1.019375</td>\n",
       "      <td>1.027825</td>\n",
       "      <td>1.028656</td>\n",
       "      <td>1.017283</td>\n",
       "      <td>1.011033</td>\n",
       "      <td>1.011946</td>\n",
       "      <td>1.012946</td>\n",
       "      <td>1.017731</td>\n",
       "      <td>...</td>\n",
       "      <td>80.062991</td>\n",
       "      <td>81.814072</td>\n",
       "      <td>80.990279</td>\n",
       "      <td>0.128431</td>\n",
       "      <td>0.111244</td>\n",
       "      <td>0.095635</td>\n",
       "      <td>0.397623</td>\n",
       "      <td>0.400925</td>\n",
       "      <td>0.402171</td>\n",
       "      <td>0.388747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.176957</td>\n",
       "      <td>1.199406</td>\n",
       "      <td>1.200742</td>\n",
       "      <td>1.220164</td>\n",
       "      <td>1.284075</td>\n",
       "      <td>1.397539</td>\n",
       "      <td>1.146124</td>\n",
       "      <td>1.156096</td>\n",
       "      <td>1.163084</td>\n",
       "      <td>1.176782</td>\n",
       "      <td>...</td>\n",
       "      <td>99.008787</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.095908</td>\n",
       "      <td>0.531284</td>\n",
       "      <td>0.512178</td>\n",
       "      <td>0.390804</td>\n",
       "      <td>7.488302</td>\n",
       "      <td>9.550685</td>\n",
       "      <td>14.012035</td>\n",
       "      <td>9.284693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  stock_SMA_100  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.998330      0.997355      0.996371      0.990446       0.979683   \n",
       "std        0.028107      0.034721      0.039930      0.060691       0.079075   \n",
       "min        0.875543      0.848016      0.825517      0.789064       0.758487   \n",
       "25%        0.982204      0.976379      0.971752      0.950291       0.926987   \n",
       "50%        0.997701      0.995857      0.994971      0.986995       0.974474   \n",
       "75%        1.013807      1.016704      1.019375      1.027825       1.028656   \n",
       "max        1.176957      1.199406      1.200742      1.220164       1.284075   \n",
       "\n",
       "       stock_SMA_200  stock_EMA_10  stock_EMA_12  stock_EMA_14  stock_EMA_26  \\\n",
       "count    2516.000000   2516.000000   2516.000000   2516.000000   2516.000000   \n",
       "mean        0.954701      0.998311      0.997924      0.997535      0.995164   \n",
       "std         0.101644      0.023539      0.026043      0.028279      0.038447   \n",
       "min         0.735855      0.886369      0.877253      0.869803      0.841344   \n",
       "25%         0.881352      0.984685      0.982559      0.980652      0.970637   \n",
       "50%         0.941972      0.997422      0.996957      0.996345      0.994610   \n",
       "75%         1.017283      1.011033      1.011946      1.012946      1.017731   \n",
       "max         1.397539      1.146124      1.156096      1.163084      1.176782   \n",
       "\n",
       "       ...  stock_Stoch_21_signal  stock_Stoch_28  stock_Stoch_28_signal  \\\n",
       "count  ...            2516.000000     2516.000000            2516.000000   \n",
       "mean   ...              54.455065       55.069771              55.063123   \n",
       "std    ...              27.815368       29.149687              27.936042   \n",
       "min    ...               1.604939        0.000000               1.432497   \n",
       "25%    ...              28.601885       29.580704              29.953366   \n",
       "50%    ...              56.681239       56.527895              57.350407   \n",
       "75%    ...              80.062991       81.814072              80.990279   \n",
       "max    ...              99.008787      100.000000              99.095908   \n",
       "\n",
       "       stock_CMF_14  stock_CMF_20  stock_CMF_28  stock_VROC_7  stock_VROC_14  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.016114      0.015653      0.015484      0.169889       0.176499   \n",
       "std        0.168749      0.142715      0.122835      0.759882       0.810442   \n",
       "min       -0.414617     -0.358564     -0.301133     -0.884136      -0.892026   \n",
       "25%       -0.109803     -0.086347     -0.071768     -0.286930      -0.293252   \n",
       "50%        0.013233      0.016852      0.014964     -0.006633      -0.022291   \n",
       "75%        0.128431      0.111244      0.095635      0.397623       0.400925   \n",
       "max        0.531284      0.512178      0.390804      7.488302       9.550685   \n",
       "\n",
       "       stock_VROC_21  stock_VROC_28  \n",
       "count    2516.000000    2516.000000  \n",
       "mean        0.170427       0.162093  \n",
       "std         0.823620       0.803365  \n",
       "min        -0.905211      -0.929906  \n",
       "25%        -0.301400      -0.310590  \n",
       "50%        -0.012426      -0.031853  \n",
       "75%         0.402171       0.388747  \n",
       "max        14.012035       9.284693  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des indicateurs\n",
    "ebay_df[INDICATORS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_target</th>\n",
       "      <th>news_neg</th>\n",
       "      <th>news_neu</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>stock_reddit_neg</th>\n",
       "      <th>stock_reddit_neu</th>\n",
       "      <th>stock_reddit_pos</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2215.000000</td>\n",
       "      <td>2215.000000</td>\n",
       "      <td>2215.000000</td>\n",
       "      <td>2240.000000</td>\n",
       "      <td>2240.000000</td>\n",
       "      <td>2240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.078552</td>\n",
       "      <td>0.795839</td>\n",
       "      <td>0.125608</td>\n",
       "      <td>0.046836</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>16.861692</td>\n",
       "      <td>0.554658</td>\n",
       "      <td>0.093720</td>\n",
       "      <td>0.850306</td>\n",
       "      <td>0.055884</td>\n",
       "      <td>0.048496</td>\n",
       "      <td>0.879064</td>\n",
       "      <td>0.071646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.785427</td>\n",
       "      <td>0.138744</td>\n",
       "      <td>0.202417</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.931054</td>\n",
       "      <td>0.995375</td>\n",
       "      <td>5.634105</td>\n",
       "      <td>0.776166</td>\n",
       "      <td>0.059160</td>\n",
       "      <td>0.059049</td>\n",
       "      <td>0.024726</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.060771</td>\n",
       "      <td>0.045480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.452691</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>0.022354</td>\n",
       "      <td>0.008707</td>\n",
       "      <td>-6.663446</td>\n",
       "      <td>-9.353766</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.009169</td>\n",
       "      <td>0.076930</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.034308</td>\n",
       "      <td>0.015812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.817519</td>\n",
       "      <td>0.021461</td>\n",
       "      <td>0.759642</td>\n",
       "      <td>0.044543</td>\n",
       "      <td>-0.326374</td>\n",
       "      <td>-0.456111</td>\n",
       "      <td>13.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.053709</td>\n",
       "      <td>0.822874</td>\n",
       "      <td>0.042122</td>\n",
       "      <td>0.027601</td>\n",
       "      <td>0.864915</td>\n",
       "      <td>0.049709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.054721</td>\n",
       "      <td>0.031933</td>\n",
       "      <td>0.883390</td>\n",
       "      <td>0.063467</td>\n",
       "      <td>0.060024</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>15.475000</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>0.862577</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.034882</td>\n",
       "      <td>0.896217</td>\n",
       "      <td>0.060088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.972312</td>\n",
       "      <td>0.059712</td>\n",
       "      <td>0.914864</td>\n",
       "      <td>0.108502</td>\n",
       "      <td>0.505720</td>\n",
       "      <td>0.535977</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.120906</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.061095</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>0.911803</td>\n",
       "      <td>0.077200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.919872</td>\n",
       "      <td>0.963946</td>\n",
       "      <td>0.945850</td>\n",
       "      <td>0.905597</td>\n",
       "      <td>4.959374</td>\n",
       "      <td>4.710198</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.408000</td>\n",
       "      <td>0.899480</td>\n",
       "      <td>0.939969</td>\n",
       "      <td>0.428514</td>\n",
       "      <td>0.849241</td>\n",
       "      <td>0.948691</td>\n",
       "      <td>0.946339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_target    news_neg    news_neu    news_pos  sp500_return_pct  \\\n",
       "count   2516.000000  582.000000  582.000000  582.000000       2512.000000   \n",
       "mean       0.066899    0.078552    0.795839    0.125608          0.046836   \n",
       "std        1.785427    0.138744    0.202417    0.165800          0.931054   \n",
       "min      -12.452691    0.010012    0.022354    0.008707         -6.663446   \n",
       "25%       -0.817519    0.021461    0.759642    0.044543         -0.326374   \n",
       "50%        0.054721    0.031933    0.883390    0.063467          0.060024   \n",
       "75%        0.972312    0.059712    0.914864    0.108502          0.505720   \n",
       "max       13.919872    0.963946    0.945850    0.905597          4.959374   \n",
       "\n",
       "       gold_return_pct    vix_close  bond_yields_close  stock_reddit_neg  \\\n",
       "count      2512.000000  2512.000000        2512.000000       2215.000000   \n",
       "mean          0.017741    16.861692           0.554658          0.093720   \n",
       "std           0.995375     5.634105           0.776166          0.059160   \n",
       "min          -9.353766     9.140000           0.003000          0.009169   \n",
       "25%          -0.456111    13.040000           0.035000          0.053709   \n",
       "50%           0.018708    15.475000           0.117500          0.079832   \n",
       "75%           0.535977    18.900000           0.985000          0.120906   \n",
       "max           4.710198    48.000000           2.408000          0.899480   \n",
       "\n",
       "       stock_reddit_neu  stock_reddit_pos  sector_reddit_neg  \\\n",
       "count       2215.000000       2215.000000        2240.000000   \n",
       "mean           0.850306          0.055884           0.048496   \n",
       "std            0.059049          0.024726           0.045700   \n",
       "min            0.076930          0.018770           0.008277   \n",
       "25%            0.822874          0.042122           0.027601   \n",
       "50%            0.862577          0.049753           0.034882   \n",
       "75%            0.890271          0.061095           0.052194   \n",
       "max            0.939969          0.428514           0.849241   \n",
       "\n",
       "       sector_reddit_neu  sector_reddit_pos  \n",
       "count        2240.000000        2240.000000  \n",
       "mean            0.879064           0.071646  \n",
       "std             0.060771           0.045480  \n",
       "min             0.034308           0.015812  \n",
       "25%             0.864915           0.049709  \n",
       "50%             0.896217           0.060088  \n",
       "75%             0.911803           0.077200  \n",
       "max             0.948691           0.946339  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des autres colonnes\n",
    "ebay_df.drop(INDICATORS, axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des Valeurs Manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant imputation:\n",
      " news_neg             1934\n",
      "news_neu             1934\n",
      "news_pos             1934\n",
      "sp500_return_pct        4\n",
      "gold_return_pct         4\n",
      "vix_close               4\n",
      "bond_yields_close       4\n",
      "stock_reddit_neg      301\n",
      "stock_reddit_neu      301\n",
      "stock_reddit_pos      301\n",
      "sector_reddit_neg     276\n",
      "sector_reddit_neu     276\n",
      "sector_reddit_pos     276\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print le nombre de valeurs manquantes\n",
    "missing_values = ebay_df.isnull().sum()\n",
    "print(\"Valeurs manquantes avant imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la médiane\n",
    "ebay_df_imputed_median = ebay_df.fillna(ebay_df.median())\n",
    "\n",
    "missing_values = ebay_df_imputed_median.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la moyenne\n",
    "ebay_df_imputed_mean = ebay_df.fillna(ebay_df.mean())\n",
    "\n",
    "missing_values = ebay_df_imputed_mean.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " news_neg             11\n",
      "news_neu             11\n",
      "news_pos             11\n",
      "stock_reddit_neg      8\n",
      "stock_reddit_neu      8\n",
      "stock_reddit_pos      8\n",
      "sector_reddit_neg     1\n",
      "sector_reddit_neu     1\n",
      "sector_reddit_pos     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec une interpolation linéaire\n",
    "ebay_df_imputed_interpolate = ebay_df.interpolate(method='linear')\n",
    "\n",
    "missing_values = ebay_df_imputed_interpolate.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " news_neg             11\n",
      "news_neu             11\n",
      "news_pos             11\n",
      "stock_reddit_neg      8\n",
      "stock_reddit_neu      8\n",
      "stock_reddit_pos      8\n",
      "sector_reddit_neg     1\n",
      "sector_reddit_neu     1\n",
      "sector_reddit_pos     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec forward fill\n",
    "ebay_df_imputed_ffill = ebay_df.ffill()\n",
    "\n",
    "missing_values = ebay_df_imputed_ffill.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, method='ffill'):\n",
    "    if method == 'median':\n",
    "        return df.fillna(df.median())\n",
    "    elif method == 'mean':\n",
    "        return df.fillna(df.mean())\n",
    "    elif method == 'interpolate':\n",
    "        return df.interpolate(method='linear')\n",
    "    elif method == 'ffill':\n",
    "        return df.ffill()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la Variable Cible (stock_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target\n",
      "1    1296\n",
      "0    1220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target = ebay_df['stock_target']\n",
    "target = target.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "target_counts = target.value_counts()\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des Données pour le Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer des variables lags pour les indicateurs\n",
    "def create_lag_variables(data, features, lags=[1, 2, 3, 4, 5, 6, 7]):\n",
    "    df = data.copy()\n",
    "    lagged_columns = {}\n",
    "\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lagged_columns[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "    \n",
    "    lagged_df = pd.DataFrame(lagged_columns, index=df.index)\n",
    "    df = pd.concat([df, lagged_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "ebay_df_lagged = create_lag_variables(ebay_df, ebay_df.columns)\n",
    "ebay_df_lagged = impute_missing_values(ebay_df_lagged, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target              1.000000\n",
      "gold_return_pct_lag_4     0.057844\n",
      "stock_EMA_10              0.054415\n",
      "stock_EMA_12              0.054230\n",
      "stock_EMA_14              0.054046\n",
      "                            ...   \n",
      "stock_Stoch_21           -0.048092\n",
      "stock_Stoch_28           -0.049257\n",
      "stock_VROC_28_lag_6      -0.051308\n",
      "stock_Stoch_14           -0.051409\n",
      "stock_reddit_neu_lag_2   -0.060317\n",
      "Name: stock_target, Length: 464, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la corrélation entre les features et la cible\n",
    "correlations = ebay_df_lagged.corr()['stock_target'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516, 464)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebay_df_lagged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection avec Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ebay_df_lagged.copy()\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['stock_target'], axis=1)\n",
    "y = data['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiser les features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>sector_reddit_neu_lag_5</th>\n",
       "      <th>sector_reddit_neu_lag_6</th>\n",
       "      <th>sector_reddit_neu_lag_7</th>\n",
       "      <th>sector_reddit_pos_lag_1</th>\n",
       "      <th>sector_reddit_pos_lag_2</th>\n",
       "      <th>sector_reddit_pos_lag_3</th>\n",
       "      <th>sector_reddit_pos_lag_4</th>\n",
       "      <th>sector_reddit_pos_lag_5</th>\n",
       "      <th>sector_reddit_pos_lag_6</th>\n",
       "      <th>sector_reddit_pos_lag_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-29</th>\n",
       "      <td>0.607947</td>\n",
       "      <td>0.499494</td>\n",
       "      <td>0.513539</td>\n",
       "      <td>0.378527</td>\n",
       "      <td>0.544610</td>\n",
       "      <td>-0.400524</td>\n",
       "      <td>0.849090</td>\n",
       "      <td>0.770851</td>\n",
       "      <td>0.712428</td>\n",
       "      <td>0.548583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830969</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>-0.170947</td>\n",
       "      <td>0.763595</td>\n",
       "      <td>-1.017985</td>\n",
       "      <td>-1.017656</td>\n",
       "      <td>-1.017081</td>\n",
       "      <td>-1.016576</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>0.811924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-01</th>\n",
       "      <td>0.481704</td>\n",
       "      <td>0.282506</td>\n",
       "      <td>0.329635</td>\n",
       "      <td>0.263930</td>\n",
       "      <td>0.468413</td>\n",
       "      <td>-0.440121</td>\n",
       "      <td>0.478958</td>\n",
       "      <td>0.450758</td>\n",
       "      <td>0.427517</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830969</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>0.830837</td>\n",
       "      <td>0.763595</td>\n",
       "      <td>0.763715</td>\n",
       "      <td>-1.017656</td>\n",
       "      <td>-1.017081</td>\n",
       "      <td>-1.016576</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>-1.016302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-02</th>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.364692</td>\n",
       "      <td>0.383613</td>\n",
       "      <td>0.326843</td>\n",
       "      <td>0.520590</td>\n",
       "      <td>-0.386739</td>\n",
       "      <td>0.541656</td>\n",
       "      <td>0.521448</td>\n",
       "      <td>0.502622</td>\n",
       "      <td>0.436457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830969</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>0.830837</td>\n",
       "      <td>-0.921081</td>\n",
       "      <td>0.763715</td>\n",
       "      <td>0.763787</td>\n",
       "      <td>-1.017081</td>\n",
       "      <td>-1.016576</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>-1.016302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-03</th>\n",
       "      <td>0.514272</td>\n",
       "      <td>0.180494</td>\n",
       "      <td>0.197801</td>\n",
       "      <td>0.220234</td>\n",
       "      <td>0.435953</td>\n",
       "      <td>-0.425454</td>\n",
       "      <td>0.229643</td>\n",
       "      <td>0.241646</td>\n",
       "      <td>0.247385</td>\n",
       "      <td>0.256463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830969</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>0.830837</td>\n",
       "      <td>-0.921081</td>\n",
       "      <td>-0.920659</td>\n",
       "      <td>0.763787</td>\n",
       "      <td>0.763998</td>\n",
       "      <td>-1.016576</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>-1.016302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-04</th>\n",
       "      <td>1.498746</td>\n",
       "      <td>1.084795</td>\n",
       "      <td>1.011186</td>\n",
       "      <td>0.788233</td>\n",
       "      <td>0.874009</td>\n",
       "      <td>-0.096273</td>\n",
       "      <td>1.411625</td>\n",
       "      <td>1.348670</td>\n",
       "      <td>1.293843</td>\n",
       "      <td>1.085759</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052461</td>\n",
       "      <td>0.830465</td>\n",
       "      <td>0.830837</td>\n",
       "      <td>-0.921081</td>\n",
       "      <td>-0.920659</td>\n",
       "      <td>-0.920344</td>\n",
       "      <td>0.763998</td>\n",
       "      <td>0.764156</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>-1.016302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-29      0.607947      0.499494      0.513539      0.378527   \n",
       "2010-02-01      0.481704      0.282506      0.329635      0.263930   \n",
       "2010-02-02      0.594203      0.364692      0.383613      0.326843   \n",
       "2010-02-03      0.514272      0.180494      0.197801      0.220234   \n",
       "2010-02-04      1.498746      1.084795      1.011186      0.788233   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-29       0.544610      -0.400524      0.849090      0.770851   \n",
       "2010-02-01       0.468413      -0.440121      0.478958      0.450758   \n",
       "2010-02-02       0.520590      -0.386739      0.541656      0.521448   \n",
       "2010-02-03       0.435953      -0.425454      0.229643      0.241646   \n",
       "2010-02-04       0.874009      -0.096273      1.411625      1.348670   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  sector_reddit_neu_lag_5  \\\n",
       "Date                                    ...                            \n",
       "2010-01-29      0.712428      0.548583  ...                 0.830969   \n",
       "2010-02-01      0.427517      0.359200  ...                 0.830969   \n",
       "2010-02-02      0.502622      0.436457  ...                 0.830969   \n",
       "2010-02-03      0.247385      0.256463  ...                 0.830969   \n",
       "2010-02-04      1.293843      1.085759  ...                -0.052461   \n",
       "\n",
       "            sector_reddit_neu_lag_6  sector_reddit_neu_lag_7  \\\n",
       "Date                                                           \n",
       "2010-01-29                 0.830465                -0.170947   \n",
       "2010-02-01                 0.830465                 0.830837   \n",
       "2010-02-02                 0.830465                 0.830837   \n",
       "2010-02-03                 0.830465                 0.830837   \n",
       "2010-02-04                 0.830465                 0.830837   \n",
       "\n",
       "            sector_reddit_pos_lag_1  sector_reddit_pos_lag_2  \\\n",
       "Date                                                           \n",
       "2010-01-29                 0.763595                -1.017985   \n",
       "2010-02-01                 0.763595                 0.763715   \n",
       "2010-02-02                -0.921081                 0.763715   \n",
       "2010-02-03                -0.921081                -0.920659   \n",
       "2010-02-04                -0.921081                -0.920659   \n",
       "\n",
       "            sector_reddit_pos_lag_3  sector_reddit_pos_lag_4  \\\n",
       "Date                                                           \n",
       "2010-01-29                -1.017656                -1.017081   \n",
       "2010-02-01                -1.017656                -1.017081   \n",
       "2010-02-02                 0.763787                -1.017081   \n",
       "2010-02-03                 0.763787                 0.763998   \n",
       "2010-02-04                -0.920344                 0.763998   \n",
       "\n",
       "            sector_reddit_pos_lag_5  sector_reddit_pos_lag_6  \\\n",
       "Date                                                           \n",
       "2010-01-29                -1.016576                -1.015856   \n",
       "2010-02-01                -1.016576                -1.015856   \n",
       "2010-02-02                -1.016576                -1.015856   \n",
       "2010-02-03                -1.016576                -1.015856   \n",
       "2010-02-04                 0.764156                -1.015856   \n",
       "\n",
       "            sector_reddit_pos_lag_7  \n",
       "Date                                 \n",
       "2010-01-29                 0.811924  \n",
       "2010-02-01                -1.016302  \n",
       "2010-02-02                -1.016302  \n",
       "2010-02-03                -1.016302  \n",
       "2010-02-04                -1.016302  \n",
       "\n",
       "[5 rows x 463 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer LASO pour la sélection des features\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 96\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01, max_iter=10000)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# nombre de features sélectionnées\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(f'Nombre de features sélectionnées: {len(selected_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indic_coef = dict(zip(X.columns, lasso.coef_))\n",
    "non_zero_coef = {k: v for k, v in indic_coef.items() if v != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les features sélectionnées\n",
    "ebay_df_selected = data[selected_features.append(pd.Index(['stock_target']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_ADX_14</th>\n",
       "      <th>stock_ADX_14_neg</th>\n",
       "      <th>stock_ADX_30_pos</th>\n",
       "      <th>stock_RSI_7</th>\n",
       "      <th>stock_Stoch_14</th>\n",
       "      <th>stock_Stoch_21</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>...</th>\n",
       "      <th>sp500_return_pct_lag_3</th>\n",
       "      <th>sp500_return_pct_lag_4</th>\n",
       "      <th>gold_return_pct_lag_2</th>\n",
       "      <th>gold_return_pct_lag_4</th>\n",
       "      <th>gold_return_pct_lag_7</th>\n",
       "      <th>vix_close_lag_4</th>\n",
       "      <th>vix_close_lag_5</th>\n",
       "      <th>vix_close_lag_6</th>\n",
       "      <th>vix_close_lag_7</th>\n",
       "      <th>stock_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-29</th>\n",
       "      <td>11.290092</td>\n",
       "      <td>19.879898</td>\n",
       "      <td>20.405439</td>\n",
       "      <td>42.944722</td>\n",
       "      <td>36.885239</td>\n",
       "      <td>36.885239</td>\n",
       "      <td>36.885239</td>\n",
       "      <td>0.759836</td>\n",
       "      <td>-0.982917</td>\n",
       "      <td>-0.055369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>0.459810</td>\n",
       "      <td>-1.229620</td>\n",
       "      <td>0.550863</td>\n",
       "      <td>-2.404133</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>22.270000</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>0.651617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-01</th>\n",
       "      <td>10.697937</td>\n",
       "      <td>18.986014</td>\n",
       "      <td>19.931036</td>\n",
       "      <td>45.737934</td>\n",
       "      <td>43.032872</td>\n",
       "      <td>43.032872</td>\n",
       "      <td>43.032872</td>\n",
       "      <td>0.436222</td>\n",
       "      <td>1.426611</td>\n",
       "      <td>1.966764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488015</td>\n",
       "      <td>-0.420320</td>\n",
       "      <td>-0.073778</td>\n",
       "      <td>0.246537</td>\n",
       "      <td>-0.863085</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>22.270000</td>\n",
       "      <td>-0.388435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-02</th>\n",
       "      <td>10.148080</td>\n",
       "      <td>18.231918</td>\n",
       "      <td>19.529830</td>\n",
       "      <td>44.222473</td>\n",
       "      <td>39.344329</td>\n",
       "      <td>39.344329</td>\n",
       "      <td>39.344329</td>\n",
       "      <td>0.104128</td>\n",
       "      <td>1.297295</td>\n",
       "      <td>1.186270</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.181774</td>\n",
       "      <td>0.488015</td>\n",
       "      <td>-0.055369</td>\n",
       "      <td>-1.229620</td>\n",
       "      <td>-1.224268</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>0.649904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-03</th>\n",
       "      <td>9.718219</td>\n",
       "      <td>17.848539</td>\n",
       "      <td>19.112422</td>\n",
       "      <td>47.598448</td>\n",
       "      <td>45.491776</td>\n",
       "      <td>45.491776</td>\n",
       "      <td>45.491776</td>\n",
       "      <td>0.388296</td>\n",
       "      <td>-0.547431</td>\n",
       "      <td>-0.536961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.982917</td>\n",
       "      <td>-1.181774</td>\n",
       "      <td>1.966764</td>\n",
       "      <td>-0.073778</td>\n",
       "      <td>0.550863</td>\n",
       "      <td>23.730000</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>-3.357722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-04</th>\n",
       "      <td>10.215153</td>\n",
       "      <td>21.000173</td>\n",
       "      <td>18.276067</td>\n",
       "      <td>34.814693</td>\n",
       "      <td>13.524625</td>\n",
       "      <td>13.524625</td>\n",
       "      <td>13.524625</td>\n",
       "      <td>0.107708</td>\n",
       "      <td>-3.114068</td>\n",
       "      <td>-4.408854</td>\n",
       "      <td>...</td>\n",
       "      <td>1.426611</td>\n",
       "      <td>-0.982917</td>\n",
       "      <td>1.186270</td>\n",
       "      <td>-0.055369</td>\n",
       "      <td>0.246537</td>\n",
       "      <td>24.620001</td>\n",
       "      <td>23.730000</td>\n",
       "      <td>23.139999</td>\n",
       "      <td>24.549999</td>\n",
       "      <td>1.158128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_ADX_14  stock_ADX_14_neg  stock_ADX_30_pos  stock_RSI_7  \\\n",
       "Date                                                                        \n",
       "2010-01-29     11.290092         19.879898         20.405439    42.944722   \n",
       "2010-02-01     10.697937         18.986014         19.931036    45.737934   \n",
       "2010-02-02     10.148080         18.231918         19.529830    44.222473   \n",
       "2010-02-03      9.718219         17.848539         19.112422    47.598448   \n",
       "2010-02-04     10.215153         21.000173         18.276067    34.814693   \n",
       "\n",
       "            stock_Stoch_14  stock_Stoch_21  stock_Stoch_28  stock_VROC_14  \\\n",
       "Date                                                                        \n",
       "2010-01-29       36.885239       36.885239       36.885239       0.759836   \n",
       "2010-02-01       43.032872       43.032872       43.032872       0.436222   \n",
       "2010-02-02       39.344329       39.344329       39.344329       0.104128   \n",
       "2010-02-03       45.491776       45.491776       45.491776       0.388296   \n",
       "2010-02-04       13.524625       13.524625       13.524625       0.107708   \n",
       "\n",
       "            sp500_return_pct  gold_return_pct  ...  sp500_return_pct_lag_3  \\\n",
       "Date                                           ...                           \n",
       "2010-01-29         -0.982917        -0.055369  ...               -0.420320   \n",
       "2010-02-01          1.426611         1.966764  ...                0.488015   \n",
       "2010-02-02          1.297295         1.186270  ...               -1.181774   \n",
       "2010-02-03         -0.547431        -0.536961  ...               -0.982917   \n",
       "2010-02-04         -3.114068        -4.408854  ...                1.426611   \n",
       "\n",
       "            sp500_return_pct_lag_4  gold_return_pct_lag_2  \\\n",
       "Date                                                        \n",
       "2010-01-29                0.459810              -1.229620   \n",
       "2010-02-01               -0.420320              -0.073778   \n",
       "2010-02-02                0.488015              -0.055369   \n",
       "2010-02-03               -1.181774               1.966764   \n",
       "2010-02-04               -0.982917               1.186270   \n",
       "\n",
       "            gold_return_pct_lag_4  gold_return_pct_lag_7  vix_close_lag_4  \\\n",
       "Date                                                                        \n",
       "2010-01-29               0.550863              -2.404133        25.410000   \n",
       "2010-02-01               0.246537              -0.863085        24.549999   \n",
       "2010-02-02              -1.229620              -1.224268        23.139999   \n",
       "2010-02-03              -0.073778               0.550863        23.730000   \n",
       "2010-02-04              -0.055369               0.246537        24.620001   \n",
       "\n",
       "            vix_close_lag_5  vix_close_lag_6  vix_close_lag_7  stock_target  \n",
       "Date                                                                         \n",
       "2010-01-29        27.309999        22.270000        18.680000      0.651617  \n",
       "2010-02-01        25.410000        27.309999        22.270000     -0.388435  \n",
       "2010-02-02        24.549999        25.410000        27.309999      0.649904  \n",
       "2010-02-03        23.139999        24.549999        25.410000     -3.357722  \n",
       "2010-02-04        23.730000        23.139999        24.549999      1.158128  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebay_df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ebay_df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.5.2 in /opt/anaconda3/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "# !pip uninstall -y scikit-learn\n",
    "!pip install \"scikit-learn==1.5.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Grid Search pour trouver les meilleurs hyperparamètres qui maximisent la métrique F1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Pour les métriques financières\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_train_test_data(data= data, start_year = '2010', train_window=5, test_window=1):\n",
    "    df = data.copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # split data into train and test\n",
    "    train = df[(df['Date'].dt.year >= int(start_year)) & (df['Date'].dt.year < int(start_year) + train_window)]\n",
    "    test = df[(df['Date'].dt.year >= int(start_year) + train_window) & (df['Date'].dt.year < int(start_year) + train_window + test_window)]\n",
    "\n",
    "    X_train = train.drop(columns=['Date', 'stock_target']).values\n",
    "    y_train_return = train['stock_target'].values\n",
    "    # y_train = np.where(y_train > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    X_test = test.drop(columns=['Date', 'stock_target']).values\n",
    "    y_test_return = test['stock_target'].values\n",
    "    # y_test = np.where(y_test > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    print(f'X_train from {train[\"Date\"].dt.date.values[0]} to {train[\"Date\"].dt.date.values[-1]}')\n",
    "    print(f'X_test from {test[\"Date\"].dt.date.values[0]} to {test[\"Date\"].dt.date.values[-1]}')\n",
    "    \n",
    "    return X_train, y_train_return, X_test, y_test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, alpha=0.01):\n",
    "    data = df.copy()\n",
    "    X = data.drop('stock_target', axis=1)\n",
    "    y = data['stock_target']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X, y)\n",
    "    selected_features = X.columns[lasso.coef_ != 0]\n",
    "    data_selected = data[selected_features.append(pd.Index(['stock_target']))]\n",
    "    print(f'Nombre de features sélectionnées: {len(selected_features)}')\n",
    "    return data_selected\n",
    "\n",
    "def xgboost_grid_search(X_train, y_train, params, num_boost_round=300):\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    recall_scorer = make_scorer(recall_score)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=params,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "def grid_search_best_params(df, params, model_grid_search):\n",
    "    data = df.copy()\n",
    "    grid_search_params = params.copy()\n",
    "    grid_search_params.pop('nan_strategy')\n",
    "    grid_search_params.pop('lasso_alpha')\n",
    "\n",
    "    best_params = {}\n",
    "    score = 0\n",
    "    for nan_strategy in params['nan_strategy']:\n",
    "        current_params = {'nan_strategy': nan_strategy}\n",
    "        data_lagged = create_lag_variables(data, data.columns)\n",
    "        data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "        data_lagged = data_lagged.dropna()\n",
    "        for lasso_alpha in params['lasso_alpha']:\n",
    "            current_params['lasso_alpha'] = lasso_alpha\n",
    "\n",
    "            # Lasso feature selection\n",
    "            data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "            X = data_selected.drop('stock_target', axis=1)\n",
    "            y = data_selected['stock_target']\n",
    "\n",
    "            # get rolling train test data\n",
    "            X_train, y_train_return, _, _ = get_rolling_train_test_data(\n",
    "                data_selected,\n",
    "                start_year='2010',\n",
    "                train_window=5,\n",
    "                test_window=1)\n",
    "            y_train = np.where(y_train_return > 0, 1, 0)\n",
    "\n",
    "            # grid search\n",
    "            best_params_, best_score_ = model_grid_search(X_train, y_train, grid_search_params)\n",
    "            if best_score_ > score:\n",
    "                best_params = current_params\n",
    "                best_params.update(best_params_)\n",
    "                score = best_score_\n",
    "            \n",
    "    return best_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.068e+00, tolerance: 7.873e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 273\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 37\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.300e+00, tolerance: 7.873e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 264\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:42:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 33\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e-01, tolerance: 7.858e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 383\n",
      "X_train from 2010-02-09 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 88\n",
      "X_train from 2010-02-09 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.419e-01, tolerance: 7.858e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 383\n",
      "X_train from 2010-02-09 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 88\n",
      "X_train from 2010-02-09 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:43:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:44:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'max_depth': [3, 4],           # Profondeur maximale de l'arbre\n",
    "    'learning_rate': [0.02, 0.01],  # Taux d'apprentissage (eta)\n",
    "    'n_estimators': [100],  # Nombre d'arbres (boost rounds)\n",
    "    'subsample': [0.5, 0.6],     # Fraction des données pour chaque arbre\n",
    "    'colsample_bytree': [0.5]  # Fraction des colonnes pour chaque arbre\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, xgboost_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'median',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'colsample_bytree': 0.5,\n",
       "  'learning_rate': 0.01,\n",
       "  'max_depth': 3,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.5},\n",
       " 0.5958752526926344)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 24\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle avec les meilleurs hyperparamètres\n",
    "data = ebay_df.copy()\n",
    "\n",
    "# Créer des variables lags\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=best_params['nan_strategy'])\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "# Feature selection avec Lasso\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=best_params['lasso_alpha'])\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:45:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2010\n",
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree']\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def random_forest_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Vous pouvez également définir d'autres métriques, par exemple :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "\n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,         # Vous pouvez augmenter le nombre de folds (k-fold cross validation)\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 103\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 23\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 105\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 20\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'median',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'max_depth': None,\n",
       "  'min_samples_leaf': 1,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 100},\n",
       " 0.591045234457281)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'n_estimators': [100, 200], # Nombre d'arbres\n",
    "    'max_depth': [None, 5, 10], # Profondeur maximale de l'arbre\n",
    "    'min_samples_split': [2, 5], # Nombre minimum d'échantillons pour diviser un nœud\n",
    "    'min_samples_leaf': [1, 2] # Nombre minimum d'échantillons requis à chaque feuille\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, random_forest_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.536\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Regression Logistique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def logistic_regression_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    # Note : pour utiliser la pénalisation L1, vous devez choisir un solver qui la supporte\n",
    "    # comme 'liblinear' ou 'saga'.\n",
    "    log_reg_model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Exemples d'autres métriques possibles :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "    # accuracy_scorer = 'accuracy'\n",
    "    \n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=log_reg_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,  # Vous pouvez mettre 'accuracy', recall_scorer, etc.\n",
    "        cv=3,               # Nombre de folds pour la cross-validation\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 103\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 23\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 105\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 20\n",
      "X_train from 2010-01-29 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'median',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'C': 0.1,\n",
       "  'penalty': 'l2',\n",
       "  'solver': 'saga'},\n",
       " 0.5814077941779171)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, logistic_regression_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.483\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "log_reg_model = LogisticRegression(random_state=42, penalty=best_params['penalty'], C=best_params['C'], solver=best_params['solver'], max_iter=10000)\n",
    "\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install tensorflow-macos\n",
    "# !pip install tensorflow-metal\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "nan_strategy = 'ffill'\n",
    "lasso_alpha = 0.01\n",
    "\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']\n",
    "\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year='2014', train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Créer le modèle\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # pour classification binaire\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    # metrics accuracy, recall, precision, f1-score\n",
    "    metrics=[keras.metrics.Recall(name='accuracy'), keras.metrics.Recall(name='recall'), keras.metrics.BinaryAccuracy(name='f1_score')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5346 - f1_score: 0.5060 - loss: 29.6663 - recall: 0.5346 - val_accuracy: 0.8321 - val_f1_score: 0.5397 - val_loss: 3.7695 - val_recall: 0.8321\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5684 - f1_score: 0.5375 - loss: 42.9519 - recall: 0.5684 - val_accuracy: 0.1679 - val_f1_score: 0.5040 - val_loss: 8.5853 - val_recall: 0.1679\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5239 - f1_score: 0.4924 - loss: 45.4549 - recall: 0.5239 - val_accuracy: 0.0876 - val_f1_score: 0.4841 - val_loss: 9.3548 - val_recall: 0.0876\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4938 - f1_score: 0.5054 - loss: 35.7810 - recall: 0.4938 - val_accuracy: 0.1241 - val_f1_score: 0.5000 - val_loss: 4.7122 - val_recall: 0.1241\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4931 - f1_score: 0.4617 - loss: 44.7645 - recall: 0.4931 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4563 - val_loss: 12.8197 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4995 - f1_score: 0.4815 - loss: 45.3063 - recall: 0.4995 - val_accuracy: 0.0073 - val_f1_score: 0.4603 - val_loss: 8.5614 - val_recall: 0.0073\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4929 - f1_score: 0.5055 - loss: 47.1834 - recall: 0.4929 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4563 - val_loss: 12.6143 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4809 - f1_score: 0.4972 - loss: 41.6769 - recall: 0.4809 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4563 - val_loss: 17.1025 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4767 - f1_score: 0.5158 - loss: 38.6311 - recall: 0.4767 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4563 - val_loss: 11.9872 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4191 - f1_score: 0.4769 - loss: 33.8582 - recall: 0.4191 - val_accuracy: 0.2701 - val_f1_score: 0.4921 - val_loss: 1.3012 - val_recall: 0.2701\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "F1 Score: 0.366\n"
     ]
    }
   ],
   "source": [
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
