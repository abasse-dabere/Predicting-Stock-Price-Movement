{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2516, 58)\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "aapl_df = pd.read_csv('../data/merged_data/AAPL.csv')\n",
    "aapl_df = aapl_df[(aapl_df['Date'] >= start_date) & (aapl_df['Date'] <= end_date)]\n",
    "\n",
    "# Convertir la colonne 'Date' en datetime et la définir comme index\n",
    "aapl_df['Date'] = pd.to_datetime(aapl_df['Date'])\n",
    "aapl_df.set_index('Date', inplace=True)\n",
    "\n",
    "print(aapl_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>stock_reddit_neg</th>\n",
       "      <th>stock_reddit_neu</th>\n",
       "      <th>stock_reddit_pos</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.963623</td>\n",
       "      <td>0.945426</td>\n",
       "      <td>0.934846</td>\n",
       "      <td>0.933008</td>\n",
       "      <td>0.883653</td>\n",
       "      <td>0.755846</td>\n",
       "      <td>0.968436</td>\n",
       "      <td>0.963540</td>\n",
       "      <td>0.959512</td>\n",
       "      <td>0.945193</td>\n",
       "      <td>...</td>\n",
       "      <td>1.604342</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.819740</td>\n",
       "      <td>0.143134</td>\n",
       "      <td>0.155312</td>\n",
       "      <td>0.796095</td>\n",
       "      <td>0.048593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.970799</td>\n",
       "      <td>0.949924</td>\n",
       "      <td>0.938145</td>\n",
       "      <td>0.932254</td>\n",
       "      <td>0.884416</td>\n",
       "      <td>0.757172</td>\n",
       "      <td>0.972808</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.963475</td>\n",
       "      <td>0.947742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311568</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.031367</td>\n",
       "      <td>0.923151</td>\n",
       "      <td>0.045481</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.913047</td>\n",
       "      <td>0.053150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.992530</td>\n",
       "      <td>0.969699</td>\n",
       "      <td>0.958527</td>\n",
       "      <td>0.947989</td>\n",
       "      <td>0.900728</td>\n",
       "      <td>0.771859</td>\n",
       "      <td>0.990617</td>\n",
       "      <td>0.985940</td>\n",
       "      <td>0.981842</td>\n",
       "      <td>0.965797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.035153</td>\n",
       "      <td>0.921260</td>\n",
       "      <td>0.043587</td>\n",
       "      <td>0.082909</td>\n",
       "      <td>0.865224</td>\n",
       "      <td>0.051867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.999221</td>\n",
       "      <td>0.976690</td>\n",
       "      <td>0.965220</td>\n",
       "      <td>0.950514</td>\n",
       "      <td>0.904477</td>\n",
       "      <td>0.775760</td>\n",
       "      <td>0.993824</td>\n",
       "      <td>0.989648</td>\n",
       "      <td>0.985839</td>\n",
       "      <td>0.969987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.052410</td>\n",
       "      <td>0.899681</td>\n",
       "      <td>0.047909</td>\n",
       "      <td>0.046438</td>\n",
       "      <td>0.908690</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.997283</td>\n",
       "      <td>0.975570</td>\n",
       "      <td>0.962190</td>\n",
       "      <td>0.945615</td>\n",
       "      <td>0.900975</td>\n",
       "      <td>0.773125</td>\n",
       "      <td>0.989577</td>\n",
       "      <td>0.985710</td>\n",
       "      <td>0.982084</td>\n",
       "      <td>0.966279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288173</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.019935</td>\n",
       "      <td>0.904986</td>\n",
       "      <td>0.075079</td>\n",
       "      <td>0.084882</td>\n",
       "      <td>0.858233</td>\n",
       "      <td>0.056885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-04      0.963623      0.945426      0.934846      0.933008   \n",
       "2010-01-05      0.970799      0.949924      0.938145      0.932254   \n",
       "2010-01-06      0.992530      0.969699      0.958527      0.947989   \n",
       "2010-01-07      0.999221      0.976690      0.965220      0.950514   \n",
       "2010-01-08      0.997283      0.975570      0.962190      0.945615   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-04       0.883653       0.755846      0.968436      0.963540   \n",
       "2010-01-05       0.884416       0.757172      0.972808      0.967742   \n",
       "2010-01-06       0.900728       0.771859      0.990617      0.985940   \n",
       "2010-01-07       0.904477       0.775760      0.993824      0.989648   \n",
       "2010-01-08       0.900975       0.773125      0.989577      0.985710   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  sp500_return_pct  \\\n",
       "Date                                    ...                     \n",
       "2010-01-04      0.959512      0.945193  ...          1.604342   \n",
       "2010-01-05      0.963475      0.947742  ...          0.311568   \n",
       "2010-01-06      0.981842      0.965797  ...          0.054552   \n",
       "2010-01-07      0.985839      0.969987  ...          0.400120   \n",
       "2010-01-08      0.982084      0.966279  ...          0.288173   \n",
       "\n",
       "            gold_return_pct  vix_close  bond_yields_close  stock_reddit_neg  \\\n",
       "Date                                                                          \n",
       "2010-01-04         2.054419  20.040001              0.055          0.037126   \n",
       "2010-01-05         0.035790  19.350000              0.060          0.031367   \n",
       "2010-01-06         1.591991  19.160000              0.045          0.035153   \n",
       "2010-01-07        -0.246505  19.059999              0.045          0.052410   \n",
       "2010-01-08         0.450091  18.129999              0.040          0.019935   \n",
       "\n",
       "            stock_reddit_neu  stock_reddit_pos  sector_reddit_neg  \\\n",
       "Date                                                                \n",
       "2010-01-04          0.819740          0.143134           0.155312   \n",
       "2010-01-05          0.923151          0.045481           0.033803   \n",
       "2010-01-06          0.921260          0.043587           0.082909   \n",
       "2010-01-07          0.899681          0.047909           0.046438   \n",
       "2010-01-08          0.904986          0.075079           0.084882   \n",
       "\n",
       "            sector_reddit_neu  sector_reddit_pos  \n",
       "Date                                              \n",
       "2010-01-04           0.796095           0.048593  \n",
       "2010-01-05           0.913047           0.053150  \n",
       "2010-01-06           0.865224           0.051867  \n",
       "2010-01-07           0.908690           0.044872  \n",
       "2010-01-08           0.858233           0.056885  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les premières lignes\n",
    "aapl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\n",
    "    'stock_SMA_10', 'stock_SMA_15', 'stock_SMA_20', 'stock_SMA_50',\n",
    "    'stock_SMA_100', 'stock_SMA_200', 'stock_EMA_10', 'stock_EMA_12',\n",
    "    'stock_EMA_14', 'stock_EMA_26', 'stock_EMA_30', 'stock_EMA_50',\n",
    "    'stock_EMA_100', 'stock_ADX_14', 'stock_ADX_14_neg', 'stock_ADX_14_pos',\n",
    "    'stock_ADX_20', 'stock_ADX_20_neg', 'stock_ADX_20_pos', 'stock_ADX_25',\n",
    "    'stock_ADX_25_neg', 'stock_ADX_25_pos', 'stock_ADX_30',\n",
    "    'stock_ADX_30_neg', 'stock_ADX_30_pos', 'stock_ATR_14', 'stock_ATR_20',\n",
    "    'stock_ATR_28', 'stock_RSI_7', 'stock_RSI_14', 'stock_RSI_21',\n",
    "    'stock_Stoch_14', 'stock_Stoch_14_signal', 'stock_Stoch_21',\n",
    "    'stock_Stoch_21_signal', 'stock_Stoch_28', 'stock_Stoch_28_signal',\n",
    "    'stock_CMF_14', 'stock_CMF_20', 'stock_CMF_28', 'stock_VROC_7',\n",
    "    'stock_VROC_14', 'stock_VROC_21', 'stock_VROC_28'\n",
    "]\n",
    "\n",
    "print(len(INDICATORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>stock_Stoch_21_signal</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_Stoch_28_signal</th>\n",
       "      <th>stock_CMF_14</th>\n",
       "      <th>stock_CMF_20</th>\n",
       "      <th>stock_CMF_28</th>\n",
       "      <th>stock_VROC_7</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>stock_VROC_21</th>\n",
       "      <th>stock_VROC_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.996534</td>\n",
       "      <td>0.994587</td>\n",
       "      <td>0.992659</td>\n",
       "      <td>0.981622</td>\n",
       "      <td>0.964131</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>0.996534</td>\n",
       "      <td>0.995768</td>\n",
       "      <td>0.995007</td>\n",
       "      <td>0.990525</td>\n",
       "      <td>...</td>\n",
       "      <td>61.552329</td>\n",
       "      <td>62.347958</td>\n",
       "      <td>62.344457</td>\n",
       "      <td>0.039995</td>\n",
       "      <td>0.038655</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.111289</td>\n",
       "      <td>0.126378</td>\n",
       "      <td>0.127097</td>\n",
       "      <td>0.120952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.027302</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>0.066427</td>\n",
       "      <td>0.094056</td>\n",
       "      <td>0.130278</td>\n",
       "      <td>0.023057</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>0.028165</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>...</td>\n",
       "      <td>29.081772</td>\n",
       "      <td>30.037666</td>\n",
       "      <td>29.044301</td>\n",
       "      <td>0.177126</td>\n",
       "      <td>0.149679</td>\n",
       "      <td>0.130196</td>\n",
       "      <td>0.576071</td>\n",
       "      <td>0.650056</td>\n",
       "      <td>0.673652</td>\n",
       "      <td>0.650932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.909796</td>\n",
       "      <td>0.900574</td>\n",
       "      <td>0.898222</td>\n",
       "      <td>0.814260</td>\n",
       "      <td>0.731557</td>\n",
       "      <td>0.671050</td>\n",
       "      <td>0.926691</td>\n",
       "      <td>0.921590</td>\n",
       "      <td>0.917837</td>\n",
       "      <td>0.886927</td>\n",
       "      <td>...</td>\n",
       "      <td>1.253199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.733466</td>\n",
       "      <td>-0.477755</td>\n",
       "      <td>-0.382779</td>\n",
       "      <td>-0.364944</td>\n",
       "      <td>-0.848471</td>\n",
       "      <td>-0.813931</td>\n",
       "      <td>-0.847502</td>\n",
       "      <td>-0.863293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.978533</td>\n",
       "      <td>0.971241</td>\n",
       "      <td>0.965414</td>\n",
       "      <td>0.936917</td>\n",
       "      <td>0.900727</td>\n",
       "      <td>0.838616</td>\n",
       "      <td>0.981375</td>\n",
       "      <td>0.978808</td>\n",
       "      <td>0.976258</td>\n",
       "      <td>0.963544</td>\n",
       "      <td>...</td>\n",
       "      <td>35.504822</td>\n",
       "      <td>38.262328</td>\n",
       "      <td>38.964877</td>\n",
       "      <td>-0.091420</td>\n",
       "      <td>-0.070655</td>\n",
       "      <td>-0.057001</td>\n",
       "      <td>-0.255740</td>\n",
       "      <td>-0.284082</td>\n",
       "      <td>-0.288909</td>\n",
       "      <td>-0.281894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.994572</td>\n",
       "      <td>0.991445</td>\n",
       "      <td>0.988382</td>\n",
       "      <td>0.974174</td>\n",
       "      <td>0.947454</td>\n",
       "      <td>0.895636</td>\n",
       "      <td>0.994204</td>\n",
       "      <td>0.993042</td>\n",
       "      <td>0.992013</td>\n",
       "      <td>0.985699</td>\n",
       "      <td>...</td>\n",
       "      <td>68.995207</td>\n",
       "      <td>69.290424</td>\n",
       "      <td>69.085304</td>\n",
       "      <td>0.034526</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.030459</td>\n",
       "      <td>-0.012342</td>\n",
       "      <td>-0.015428</td>\n",
       "      <td>-0.021469</td>\n",
       "      <td>-0.032437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.011811</td>\n",
       "      <td>1.013812</td>\n",
       "      <td>1.014854</td>\n",
       "      <td>1.016012</td>\n",
       "      <td>1.004877</td>\n",
       "      <td>0.975676</td>\n",
       "      <td>1.009123</td>\n",
       "      <td>1.010031</td>\n",
       "      <td>1.010614</td>\n",
       "      <td>1.012547</td>\n",
       "      <td>...</td>\n",
       "      <td>88.265560</td>\n",
       "      <td>89.573893</td>\n",
       "      <td>89.411457</td>\n",
       "      <td>0.161169</td>\n",
       "      <td>0.148712</td>\n",
       "      <td>0.126036</td>\n",
       "      <td>0.337975</td>\n",
       "      <td>0.351918</td>\n",
       "      <td>0.341566</td>\n",
       "      <td>0.349253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.119828</td>\n",
       "      <td>1.143148</td>\n",
       "      <td>1.158347</td>\n",
       "      <td>1.303117</td>\n",
       "      <td>1.420194</td>\n",
       "      <td>1.414387</td>\n",
       "      <td>1.109108</td>\n",
       "      <td>1.118108</td>\n",
       "      <td>1.125690</td>\n",
       "      <td>1.174955</td>\n",
       "      <td>...</td>\n",
       "      <td>99.165374</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.338644</td>\n",
       "      <td>0.551720</td>\n",
       "      <td>0.466915</td>\n",
       "      <td>0.372831</td>\n",
       "      <td>5.261044</td>\n",
       "      <td>9.692106</td>\n",
       "      <td>8.827986</td>\n",
       "      <td>6.595671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  stock_SMA_100  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.996534      0.994587      0.992659      0.981622       0.964131   \n",
       "std        0.027302      0.034098      0.039913      0.066427       0.094056   \n",
       "min        0.909796      0.900574      0.898222      0.814260       0.731557   \n",
       "25%        0.978533      0.971241      0.965414      0.936917       0.900727   \n",
       "50%        0.994572      0.991445      0.988382      0.974174       0.947454   \n",
       "75%        1.011811      1.013812      1.014854      1.016012       1.004877   \n",
       "max        1.119828      1.143148      1.158347      1.303117       1.420194   \n",
       "\n",
       "       stock_SMA_200  stock_EMA_10  stock_EMA_12  stock_EMA_14  stock_EMA_26  \\\n",
       "count    2516.000000   2516.000000   2516.000000   2516.000000   2516.000000   \n",
       "mean        0.929688      0.996534      0.995768      0.995007      0.990525   \n",
       "std         0.130278      0.023057      0.025725      0.028165      0.040020   \n",
       "min         0.671050      0.926691      0.921590      0.917837      0.886927   \n",
       "25%         0.838616      0.981375      0.978808      0.976258      0.963544   \n",
       "50%         0.895636      0.994204      0.993042      0.992013      0.985699   \n",
       "75%         0.975676      1.009123      1.010031      1.010614      1.012547   \n",
       "max         1.414387      1.109108      1.118108      1.125690      1.174955   \n",
       "\n",
       "       ...  stock_Stoch_21_signal  stock_Stoch_28  stock_Stoch_28_signal  \\\n",
       "count  ...            2516.000000     2516.000000            2516.000000   \n",
       "mean   ...              61.552329       62.347958              62.344457   \n",
       "std    ...              29.081772       30.037666              29.044301   \n",
       "min    ...               1.253199        0.000000               1.733466   \n",
       "25%    ...              35.504822       38.262328              38.964877   \n",
       "50%    ...              68.995207       69.290424              69.085304   \n",
       "75%    ...              88.265560       89.573893              89.411457   \n",
       "max    ...              99.165374      100.000000              99.338644   \n",
       "\n",
       "       stock_CMF_14  stock_CMF_20  stock_CMF_28  stock_VROC_7  stock_VROC_14  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.039995      0.038655      0.036773      0.111289       0.126378   \n",
       "std        0.177126      0.149679      0.130196      0.576071       0.650056   \n",
       "min       -0.477755     -0.382779     -0.364944     -0.848471      -0.813931   \n",
       "25%       -0.091420     -0.070655     -0.057001     -0.255740      -0.284082   \n",
       "50%        0.034526      0.027419      0.030459     -0.012342      -0.015428   \n",
       "75%        0.161169      0.148712      0.126036      0.337975       0.351918   \n",
       "max        0.551720      0.466915      0.372831      5.261044       9.692106   \n",
       "\n",
       "       stock_VROC_21  stock_VROC_28  \n",
       "count    2516.000000    2516.000000  \n",
       "mean        0.127097       0.120952  \n",
       "std         0.673652       0.650932  \n",
       "min        -0.847502      -0.863293  \n",
       "25%        -0.288909      -0.281894  \n",
       "50%        -0.021469      -0.032437  \n",
       "75%         0.341566       0.349253  \n",
       "max         8.827986       6.595671  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des indicateurs\n",
    "aapl_df[INDICATORS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_target</th>\n",
       "      <th>news_neg</th>\n",
       "      <th>news_neu</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>stock_reddit_neg</th>\n",
       "      <th>stock_reddit_neu</th>\n",
       "      <th>stock_reddit_pos</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2072.000000</td>\n",
       "      <td>2072.000000</td>\n",
       "      <td>2072.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104052</td>\n",
       "      <td>0.133296</td>\n",
       "      <td>0.754822</td>\n",
       "      <td>0.111882</td>\n",
       "      <td>0.046836</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>16.861692</td>\n",
       "      <td>0.554658</td>\n",
       "      <td>0.067742</td>\n",
       "      <td>0.845922</td>\n",
       "      <td>0.085542</td>\n",
       "      <td>0.133893</td>\n",
       "      <td>0.788055</td>\n",
       "      <td>0.078052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.623597</td>\n",
       "      <td>0.169353</td>\n",
       "      <td>0.187062</td>\n",
       "      <td>0.115457</td>\n",
       "      <td>0.931054</td>\n",
       "      <td>0.995375</td>\n",
       "      <td>5.634105</td>\n",
       "      <td>0.776166</td>\n",
       "      <td>0.041442</td>\n",
       "      <td>0.051882</td>\n",
       "      <td>0.035737</td>\n",
       "      <td>0.073635</td>\n",
       "      <td>0.075993</td>\n",
       "      <td>0.033939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.355785</td>\n",
       "      <td>0.009295</td>\n",
       "      <td>0.019036</td>\n",
       "      <td>0.007598</td>\n",
       "      <td>-6.663446</td>\n",
       "      <td>-9.353766</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.018676</td>\n",
       "      <td>0.591457</td>\n",
       "      <td>0.033084</td>\n",
       "      <td>0.018808</td>\n",
       "      <td>0.442899</td>\n",
       "      <td>0.026085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.691267</td>\n",
       "      <td>0.029513</td>\n",
       "      <td>0.674559</td>\n",
       "      <td>0.048351</td>\n",
       "      <td>-0.326374</td>\n",
       "      <td>-0.456111</td>\n",
       "      <td>13.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.036994</td>\n",
       "      <td>0.816677</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>0.077159</td>\n",
       "      <td>0.741247</td>\n",
       "      <td>0.053604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.089511</td>\n",
       "      <td>0.057951</td>\n",
       "      <td>0.824043</td>\n",
       "      <td>0.071023</td>\n",
       "      <td>0.060024</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>15.475000</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.052721</td>\n",
       "      <td>0.856306</td>\n",
       "      <td>0.075660</td>\n",
       "      <td>0.123720</td>\n",
       "      <td>0.799101</td>\n",
       "      <td>0.068546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.979748</td>\n",
       "      <td>0.170293</td>\n",
       "      <td>0.892132</td>\n",
       "      <td>0.125862</td>\n",
       "      <td>0.505720</td>\n",
       "      <td>0.535977</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.090252</td>\n",
       "      <td>0.886902</td>\n",
       "      <td>0.100623</td>\n",
       "      <td>0.178913</td>\n",
       "      <td>0.844929</td>\n",
       "      <td>0.092884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.874128</td>\n",
       "      <td>0.967384</td>\n",
       "      <td>0.946458</td>\n",
       "      <td>0.932956</td>\n",
       "      <td>4.959374</td>\n",
       "      <td>4.710198</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.408000</td>\n",
       "      <td>0.277392</td>\n",
       "      <td>0.926306</td>\n",
       "      <td>0.338050</td>\n",
       "      <td>0.425087</td>\n",
       "      <td>0.924679</td>\n",
       "      <td>0.280869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_target     news_neg     news_neu     news_pos  sp500_return_pct  \\\n",
       "count   2516.000000  2072.000000  2072.000000  2072.000000       2512.000000   \n",
       "mean       0.104052     0.133296     0.754822     0.111882          0.046836   \n",
       "std        1.623597     0.169353     0.187062     0.115457          0.931054   \n",
       "min      -12.355785     0.009295     0.019036     0.007598         -6.663446   \n",
       "25%       -0.691267     0.029513     0.674559     0.048351         -0.326374   \n",
       "50%        0.089511     0.057951     0.824043     0.071023          0.060024   \n",
       "75%        0.979748     0.170293     0.892132     0.125862          0.505720   \n",
       "max        8.874128     0.967384     0.946458     0.932956          4.959374   \n",
       "\n",
       "       gold_return_pct    vix_close  bond_yields_close  stock_reddit_neg  \\\n",
       "count      2512.000000  2512.000000        2512.000000       2516.000000   \n",
       "mean          0.017741    16.861692           0.554658          0.067742   \n",
       "std           0.995375     5.634105           0.776166          0.041442   \n",
       "min          -9.353766     9.140000           0.003000          0.018676   \n",
       "25%          -0.456111    13.040000           0.035000          0.036994   \n",
       "50%           0.018708    15.475000           0.117500          0.052721   \n",
       "75%           0.535977    18.900000           0.985000          0.090252   \n",
       "max           4.710198    48.000000           2.408000          0.277392   \n",
       "\n",
       "       stock_reddit_neu  stock_reddit_pos  sector_reddit_neg  \\\n",
       "count       2516.000000       2516.000000        2516.000000   \n",
       "mean           0.845922          0.085542           0.133893   \n",
       "std            0.051882          0.035737           0.073635   \n",
       "min            0.591457          0.033084           0.018808   \n",
       "25%            0.816677          0.060604           0.077159   \n",
       "50%            0.856306          0.075660           0.123720   \n",
       "75%            0.886902          0.100623           0.178913   \n",
       "max            0.926306          0.338050           0.425087   \n",
       "\n",
       "       sector_reddit_neu  sector_reddit_pos  \n",
       "count        2516.000000        2516.000000  \n",
       "mean            0.788055           0.078052  \n",
       "std             0.075993           0.033939  \n",
       "min             0.442899           0.026085  \n",
       "25%             0.741247           0.053604  \n",
       "50%             0.799101           0.068546  \n",
       "75%             0.844929           0.092884  \n",
       "max             0.924679           0.280869  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des autres colonnes\n",
    "aapl_df.drop(INDICATORS, axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des Valeurs Manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant imputation:\n",
      " news_neg             444\n",
      "news_neu             444\n",
      "news_pos             444\n",
      "sp500_return_pct       4\n",
      "gold_return_pct        4\n",
      "vix_close              4\n",
      "bond_yields_close      4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print le nombre de valeurs manquantes\n",
    "missing_values = aapl_df.isnull().sum()\n",
    "print(\"Valeurs manquantes avant imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la médiane\n",
    "aapl_df_imputed_median = aapl_df.fillna(aapl_df.median())\n",
    "\n",
    "missing_values = aapl_df_imputed_median.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la moyenne\n",
    "aapl_df_imputed_mean = aapl_df.fillna(aapl_df.mean())\n",
    "\n",
    "missing_values = aapl_df_imputed_mean.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec une interpolation linéaire\n",
    "aapl_df_imputed_interpolate = aapl_df.interpolate(method='linear')\n",
    "\n",
    "missing_values = aapl_df_imputed_interpolate.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec forward fill\n",
    "aapl_df_imputed_ffill = aapl_df.ffill()\n",
    "\n",
    "missing_values = aapl_df_imputed_ffill.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, method='ffill'):\n",
    "    if method == 'median':\n",
    "        return df.fillna(df.median())\n",
    "    elif method == 'mean':\n",
    "        return df.fillna(df.mean())\n",
    "    elif method == 'interpolate':\n",
    "        return df.interpolate(method='linear')\n",
    "    elif method == 'ffill':\n",
    "        return df.ffill()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la Variable Cible (stock_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target\n",
      "1    1330\n",
      "0    1186\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target = aapl_df['stock_target']\n",
    "target = target.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "target_counts = target.value_counts()\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des Données pour le Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer des variables lags pour les indicateurs\n",
    "def create_lag_variables(data, features, lags=[1, 2, 3, 4, 5, 6, 7]):\n",
    "    df = data.copy()\n",
    "    lagged_columns = {}\n",
    "\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lagged_columns[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "    \n",
    "    lagged_df = pd.DataFrame(lagged_columns, index=df.index)\n",
    "    df = pd.concat([df, lagged_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "aapl_df_lagged = create_lag_variables(aapl_df, aapl_df.columns)\n",
    "aapl_df_lagged = impute_missing_values(aapl_df_lagged, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target              1.000000\n",
      "stock_VROC_28_lag_2       0.058583\n",
      "stock_VROC_28_lag_1       0.054718\n",
      "stock_VROC_28_lag_3       0.052203\n",
      "stock_reddit_neu_lag_3    0.050971\n",
      "                            ...   \n",
      "sp500_return_pct_lag_4   -0.042623\n",
      "stock_VROC_14_lag_6      -0.047903\n",
      "stock_VROC_28_lag_6      -0.051079\n",
      "stock_reddit_neg_lag_3   -0.052926\n",
      "stock_VROC_7_lag_6       -0.063071\n",
      "Name: stock_target, Length: 464, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la corrélation entre les features et la cible\n",
    "correlations = aapl_df_lagged.corr()['stock_target'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516, 464)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_df_lagged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection avec Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aapl_df_lagged.copy()\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['stock_target'], axis=1)\n",
    "y = data['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiser les features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>sector_reddit_neu_lag_5</th>\n",
       "      <th>sector_reddit_neu_lag_6</th>\n",
       "      <th>sector_reddit_neu_lag_7</th>\n",
       "      <th>sector_reddit_pos_lag_1</th>\n",
       "      <th>sector_reddit_pos_lag_2</th>\n",
       "      <th>sector_reddit_pos_lag_3</th>\n",
       "      <th>sector_reddit_pos_lag_4</th>\n",
       "      <th>sector_reddit_pos_lag_5</th>\n",
       "      <th>sector_reddit_pos_lag_6</th>\n",
       "      <th>sector_reddit_pos_lag_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>0.235393</td>\n",
       "      <td>0.019917</td>\n",
       "      <td>-0.373206</td>\n",
       "      <td>-0.380081</td>\n",
       "      <td>-0.545150</td>\n",
       "      <td>-1.112614</td>\n",
       "      <td>-0.053019</td>\n",
       "      <td>-0.107942</td>\n",
       "      <td>-0.160954</td>\n",
       "      <td>-0.340597</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017705</td>\n",
       "      <td>1.648300</td>\n",
       "      <td>0.102527</td>\n",
       "      <td>-0.812360</td>\n",
       "      <td>-0.087765</td>\n",
       "      <td>-0.624527</td>\n",
       "      <td>-0.978201</td>\n",
       "      <td>-0.771669</td>\n",
       "      <td>-0.733323</td>\n",
       "      <td>-0.867078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.410636</td>\n",
       "      <td>0.274583</td>\n",
       "      <td>-0.139210</td>\n",
       "      <td>-0.267370</td>\n",
       "      <td>-0.468233</td>\n",
       "      <td>-1.058338</td>\n",
       "      <td>0.189396</td>\n",
       "      <td>0.123944</td>\n",
       "      <td>0.061412</td>\n",
       "      <td>-0.166359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.592126</td>\n",
       "      <td>1.016357</td>\n",
       "      <td>1.648146</td>\n",
       "      <td>-1.184986</td>\n",
       "      <td>-0.812375</td>\n",
       "      <td>-0.086874</td>\n",
       "      <td>-0.624292</td>\n",
       "      <td>-0.977988</td>\n",
       "      <td>-0.771141</td>\n",
       "      <td>-0.732711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>0.952289</td>\n",
       "      <td>0.811291</td>\n",
       "      <td>0.347472</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>-0.283010</td>\n",
       "      <td>-0.936740</td>\n",
       "      <td>0.785424</td>\n",
       "      <td>0.688164</td>\n",
       "      <td>0.597713</td>\n",
       "      <td>0.250091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925320</td>\n",
       "      <td>1.590729</td>\n",
       "      <td>1.016126</td>\n",
       "      <td>0.652430</td>\n",
       "      <td>-1.185001</td>\n",
       "      <td>-0.811856</td>\n",
       "      <td>-0.086706</td>\n",
       "      <td>-0.623687</td>\n",
       "      <td>-0.977447</td>\n",
       "      <td>-0.770525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>-0.615915</td>\n",
       "      <td>-0.404076</td>\n",
       "      <td>-0.585930</td>\n",
       "      <td>-0.587711</td>\n",
       "      <td>-0.682567</td>\n",
       "      <td>-1.181495</td>\n",
       "      <td>-0.854706</td>\n",
       "      <td>-0.804122</td>\n",
       "      <td>-0.776979</td>\n",
       "      <td>-0.731318</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981294</td>\n",
       "      <td>0.923981</td>\n",
       "      <td>1.590568</td>\n",
       "      <td>-0.479490</td>\n",
       "      <td>0.652418</td>\n",
       "      <td>-1.184674</td>\n",
       "      <td>-0.811599</td>\n",
       "      <td>-0.085505</td>\n",
       "      <td>-0.623168</td>\n",
       "      <td>-0.976809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-20</th>\n",
       "      <td>-0.101084</td>\n",
       "      <td>0.046648</td>\n",
       "      <td>-0.109811</td>\n",
       "      <td>-0.340892</td>\n",
       "      <td>-0.510805</td>\n",
       "      <td>-1.070889</td>\n",
       "      <td>-0.130580</td>\n",
       "      <td>-0.154164</td>\n",
       "      <td>-0.182038</td>\n",
       "      <td>-0.312212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.311688</td>\n",
       "      <td>0.979950</td>\n",
       "      <td>0.923738</td>\n",
       "      <td>0.055792</td>\n",
       "      <td>-0.479503</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>-1.184371</td>\n",
       "      <td>-0.811201</td>\n",
       "      <td>-0.085020</td>\n",
       "      <td>-0.622568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-13      0.235393      0.019917     -0.373206     -0.380081   \n",
       "2010-01-14      0.410636      0.274583     -0.139210     -0.267370   \n",
       "2010-01-15      0.952289      0.811291      0.347472      0.004169   \n",
       "2010-01-19     -0.615915     -0.404076     -0.585930     -0.587711   \n",
       "2010-01-20     -0.101084      0.046648     -0.109811     -0.340892   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-13      -0.545150      -1.112614     -0.053019     -0.107942   \n",
       "2010-01-14      -0.468233      -1.058338      0.189396      0.123944   \n",
       "2010-01-15      -0.283010      -0.936740      0.785424      0.688164   \n",
       "2010-01-19      -0.682567      -1.181495     -0.854706     -0.804122   \n",
       "2010-01-20      -0.510805      -1.070889     -0.130580     -0.154164   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  sector_reddit_neu_lag_5  \\\n",
       "Date                                    ...                            \n",
       "2010-01-13     -0.160954     -0.340597  ...                 1.017705   \n",
       "2010-01-14      0.061412     -0.166359  ...                 1.592126   \n",
       "2010-01-15      0.597713      0.250091  ...                 0.925320   \n",
       "2010-01-19     -0.776979     -0.731318  ...                 0.981294   \n",
       "2010-01-20     -0.182038     -0.312212  ...                 1.311688   \n",
       "\n",
       "            sector_reddit_neu_lag_6  sector_reddit_neu_lag_7  \\\n",
       "Date                                                           \n",
       "2010-01-13                 1.648300                 0.102527   \n",
       "2010-01-14                 1.016357                 1.648146   \n",
       "2010-01-15                 1.590729                 1.016126   \n",
       "2010-01-19                 0.923981                 1.590568   \n",
       "2010-01-20                 0.979950                 0.923738   \n",
       "\n",
       "            sector_reddit_pos_lag_1  sector_reddit_pos_lag_2  \\\n",
       "Date                                                           \n",
       "2010-01-13                -0.812360                -0.087765   \n",
       "2010-01-14                -1.184986                -0.812375   \n",
       "2010-01-15                 0.652430                -1.185001   \n",
       "2010-01-19                -0.479490                 0.652418   \n",
       "2010-01-20                 0.055792                -0.479503   \n",
       "\n",
       "            sector_reddit_pos_lag_3  sector_reddit_pos_lag_4  \\\n",
       "Date                                                           \n",
       "2010-01-13                -0.624527                -0.978201   \n",
       "2010-01-14                -0.086874                -0.624292   \n",
       "2010-01-15                -0.811856                -0.086706   \n",
       "2010-01-19                -1.184674                -0.811599   \n",
       "2010-01-20                 0.653689                -1.184371   \n",
       "\n",
       "            sector_reddit_pos_lag_5  sector_reddit_pos_lag_6  \\\n",
       "Date                                                           \n",
       "2010-01-13                -0.771669                -0.733323   \n",
       "2010-01-14                -0.977988                -0.771141   \n",
       "2010-01-15                -0.623687                -0.977447   \n",
       "2010-01-19                -0.085505                -0.623168   \n",
       "2010-01-20                -0.811201                -0.085020   \n",
       "\n",
       "            sector_reddit_pos_lag_7  \n",
       "Date                                 \n",
       "2010-01-13                -0.867078  \n",
       "2010-01-14                -0.732711  \n",
       "2010-01-15                -0.770525  \n",
       "2010-01-19                -0.976809  \n",
       "2010-01-20                -0.622568  \n",
       "\n",
       "[5 rows x 463 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer LASO pour la sélection des features\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 95\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01, max_iter=10000)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# nombre de features sélectionnées\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(f'Nombre de features sélectionnées: {len(selected_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indic_coef = dict(zip(X.columns, lasso.coef_))\n",
    "non_zero_coef = {k: v for k, v in indic_coef.items() if v != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les features sélectionnées\n",
    "aapl_df_selected = data[selected_features.append(pd.Index(['stock_target']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_ADX_14</th>\n",
       "      <th>stock_ADX_14_pos</th>\n",
       "      <th>stock_ADX_20_neg</th>\n",
       "      <th>stock_ADX_30</th>\n",
       "      <th>stock_RSI_7</th>\n",
       "      <th>stock_RSI_21</th>\n",
       "      <th>stock_Stoch_14</th>\n",
       "      <th>stock_Stoch_21</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_return_pct_lag_2</th>\n",
       "      <th>gold_return_pct_lag_4</th>\n",
       "      <th>gold_return_pct_lag_6</th>\n",
       "      <th>gold_return_pct_lag_7</th>\n",
       "      <th>vix_close_lag_1</th>\n",
       "      <th>vix_close_lag_2</th>\n",
       "      <th>vix_close_lag_4</th>\n",
       "      <th>vix_close_lag_6</th>\n",
       "      <th>vix_close_lag_7</th>\n",
       "      <th>stock_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>20.262939</td>\n",
       "      <td>22.967592</td>\n",
       "      <td>21.496970</td>\n",
       "      <td>15.912205</td>\n",
       "      <td>56.479593</td>\n",
       "      <td>58.118319</td>\n",
       "      <td>66.576332</td>\n",
       "      <td>79.910455</td>\n",
       "      <td>81.642450</td>\n",
       "      <td>0.733519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>-0.579154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>19.004670</td>\n",
       "      <td>22.295619</td>\n",
       "      <td>21.066462</td>\n",
       "      <td>15.669494</td>\n",
       "      <td>51.756660</td>\n",
       "      <td>56.665323</td>\n",
       "      <td>49.673195</td>\n",
       "      <td>74.949138</td>\n",
       "      <td>77.108867</td>\n",
       "      <td>0.252857</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.894493</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>-1.671203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>18.359467</td>\n",
       "      <td>20.072353</td>\n",
       "      <td>23.405833</td>\n",
       "      <td>15.238556</td>\n",
       "      <td>40.438524</td>\n",
       "      <td>52.696729</td>\n",
       "      <td>15.926798</td>\n",
       "      <td>60.715711</td>\n",
       "      <td>64.102556</td>\n",
       "      <td>0.186026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>4.423831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>17.365634</td>\n",
       "      <td>22.837199</td>\n",
       "      <td>20.947355</td>\n",
       "      <td>15.016756</td>\n",
       "      <td>64.207076</td>\n",
       "      <td>60.296310</td>\n",
       "      <td>95.213118</td>\n",
       "      <td>97.763267</td>\n",
       "      <td>97.954583</td>\n",
       "      <td>0.132557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545578</td>\n",
       "      <td>-1.894493</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>-1.539240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-20</th>\n",
       "      <td>16.538072</td>\n",
       "      <td>21.245719</td>\n",
       "      <td>19.536115</td>\n",
       "      <td>14.821223</td>\n",
       "      <td>54.917381</td>\n",
       "      <td>56.814126</td>\n",
       "      <td>66.405603</td>\n",
       "      <td>83.210127</td>\n",
       "      <td>84.731036</td>\n",
       "      <td>0.374989</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.093996</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>-1.728631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_ADX_14  stock_ADX_14_pos  stock_ADX_20_neg  stock_ADX_30  \\\n",
       "Date                                                                         \n",
       "2010-01-13     20.262939         22.967592         21.496970     15.912205   \n",
       "2010-01-14     19.004670         22.295619         21.066462     15.669494   \n",
       "2010-01-15     18.359467         20.072353         23.405833     15.238556   \n",
       "2010-01-19     17.365634         22.837199         20.947355     15.016756   \n",
       "2010-01-20     16.538072         21.245719         19.536115     14.821223   \n",
       "\n",
       "            stock_RSI_7  stock_RSI_21  stock_Stoch_14  stock_Stoch_21  \\\n",
       "Date                                                                    \n",
       "2010-01-13    56.479593     58.118319       66.576332       79.910455   \n",
       "2010-01-14    51.756660     56.665323       49.673195       74.949138   \n",
       "2010-01-15    40.438524     52.696729       15.926798       60.715711   \n",
       "2010-01-19    64.207076     60.296310       95.213118       97.763267   \n",
       "2010-01-20    54.917381     56.814126       66.405603       83.210127   \n",
       "\n",
       "            stock_Stoch_28  stock_VROC_14  ...  gold_return_pct_lag_2  \\\n",
       "Date                                       ...                          \n",
       "2010-01-13       81.642450       0.733519  ...               1.098225   \n",
       "2010-01-14       77.108867       0.252857  ...              -1.894493   \n",
       "2010-01-15       64.102556       0.186026  ...               0.664364   \n",
       "2010-01-19       97.954583       0.132557  ...               0.545578   \n",
       "2010-01-20       84.731036       0.374989  ...              -1.093996   \n",
       "\n",
       "            gold_return_pct_lag_4  gold_return_pct_lag_6  \\\n",
       "Date                                                       \n",
       "2010-01-13              -0.246505               0.035790   \n",
       "2010-01-14               0.450091               1.591991   \n",
       "2010-01-15               1.098225              -0.246505   \n",
       "2010-01-19              -1.894493               0.450091   \n",
       "2010-01-20               0.664364               1.098225   \n",
       "\n",
       "            gold_return_pct_lag_7  vix_close_lag_1  vix_close_lag_2  \\\n",
       "Date                                                                  \n",
       "2010-01-13               2.054419        18.250000        17.549999   \n",
       "2010-01-14               0.035790        17.850000        18.250000   \n",
       "2010-01-15               1.591991        17.629999        17.850000   \n",
       "2010-01-19              -0.246505        17.910000        17.629999   \n",
       "2010-01-20               0.450091        17.580000        17.910000   \n",
       "\n",
       "            vix_close_lag_4  vix_close_lag_6  vix_close_lag_7  stock_target  \n",
       "Date                                                                         \n",
       "2010-01-13        19.059999        19.350000        20.040001     -0.579154  \n",
       "2010-01-14        18.129999        19.160000        19.350000     -1.671203  \n",
       "2010-01-15        17.549999        19.059999        19.160000      4.423831  \n",
       "2010-01-19        18.250000        18.129999        19.059999     -1.539240  \n",
       "2010-01-20        17.850000        17.549999        18.129999     -1.728631  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aapl_df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip uninstall -y scikit-learn\n",
    "# !pip install \"scikit-learn==1.5.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Grid Search pour trouver les meilleurs hyperparamètres qui maximisent la métrique F1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Pour les métriques financières\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_train_test_data(data= data, start_year = '2010', train_window=5, test_window=1):\n",
    "    df = data.copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # split data into train and test\n",
    "    train = df[(df['Date'].dt.year >= int(start_year)) & (df['Date'].dt.year < int(start_year) + train_window)]\n",
    "    test = df[(df['Date'].dt.year >= int(start_year) + train_window) & (df['Date'].dt.year < int(start_year) + train_window + test_window)]\n",
    "\n",
    "    X_train = train.drop(columns=['Date', 'stock_target']).values\n",
    "    y_train_return = train['stock_target'].values\n",
    "    # y_train = np.where(y_train > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    X_test = test.drop(columns=['Date', 'stock_target']).values\n",
    "    y_test_return = test['stock_target'].values\n",
    "    # y_test = np.where(y_test > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    print(f'X_train from {train[\"Date\"].dt.date.values[0]} to {train[\"Date\"].dt.date.values[-1]}')\n",
    "    print(f'X_test from {test[\"Date\"].dt.date.values[0]} to {test[\"Date\"].dt.date.values[-1]}')\n",
    "    \n",
    "    return X_train, y_train_return, X_test, y_test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, alpha=0.01):\n",
    "    data = df.copy()\n",
    "    X = data.drop('stock_target', axis=1)\n",
    "    y = data['stock_target']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X, y)\n",
    "    selected_features = X.columns[lasso.coef_ != 0]\n",
    "    data_selected = data[selected_features.append(pd.Index(['stock_target']))]\n",
    "    print(f'Nombre de features sélectionnées: {len(selected_features)}')\n",
    "    return data_selected\n",
    "\n",
    "def xgboost_grid_search(X_train, y_train, params, num_boost_round=300):\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    recall_scorer = make_scorer(recall_score)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=params,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "def grid_search_best_params(df, params, model_grid_search):\n",
    "    data = df.copy()\n",
    "    grid_search_params = params.copy()\n",
    "    grid_search_params.pop('nan_strategy')\n",
    "    grid_search_params.pop('lasso_alpha')\n",
    "\n",
    "    best_params = {}\n",
    "    score = 0\n",
    "    for nan_strategy in params['nan_strategy']:\n",
    "        current_params = {'nan_strategy': nan_strategy}\n",
    "        data_lagged = create_lag_variables(data, data.columns)\n",
    "        data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "        data_lagged = data_lagged.dropna()\n",
    "        for lasso_alpha in params['lasso_alpha']:\n",
    "            current_params['lasso_alpha'] = lasso_alpha\n",
    "\n",
    "            # Lasso feature selection\n",
    "            data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "            X = data_selected.drop('stock_target', axis=1)\n",
    "            y = data_selected['stock_target']\n",
    "\n",
    "            # get rolling train test data\n",
    "            X_train, y_train_return, _, _ = get_rolling_train_test_data(\n",
    "                data_selected,\n",
    "                start_year='2010',\n",
    "                train_window=5,\n",
    "                test_window=1)\n",
    "            y_train = np.where(y_train_return > 0, 1, 0)\n",
    "\n",
    "            # grid search\n",
    "            best_params_, best_score_ = model_grid_search(X_train, y_train, grid_search_params)\n",
    "            if best_score_ > score:\n",
    "                best_params = current_params\n",
    "                best_params.update(best_params_)\n",
    "                score = best_score_\n",
    "            \n",
    "    return best_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.367e+02, tolerance: 6.622e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 199\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 46\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:30:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.946e+01, tolerance: 6.622e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 192\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 45\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.253e+00, tolerance: 6.562e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 345\n",
      "X_train from 2010-01-25 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 93\n",
      "X_train from 2010-01-25 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:31:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.253e+00, tolerance: 6.562e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 345\n",
      "X_train from 2010-01-25 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 93\n",
      "X_train from 2010-01-25 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'max_depth': [3, 4],           # Profondeur maximale de l'arbre\n",
    "    'learning_rate': [0.02, 0.01],  # Taux d'apprentissage (eta)\n",
    "    'n_estimators': [100],  # Nombre d'arbres (boost rounds)\n",
    "    'subsample': [0.5, 0.6],     # Fraction des données pour chaque arbre\n",
    "    'colsample_bytree': [0.5]  # Fraction des colonnes pour chaque arbre\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, xgboost_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'interpolate',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'colsample_bytree': 0.5,\n",
       "  'learning_rate': 0.01,\n",
       "  'max_depth': 3,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.5},\n",
       " 0.6306640582011039)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 24\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle avec les meilleurs hyperparamètres\n",
    "data = aapl_df.copy()\n",
    "\n",
    "# Créer des variables lags\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=best_params['nan_strategy'])\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "# Feature selection avec Lasso\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=best_params['lasso_alpha'])\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.6862170087976539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:32:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2010\n",
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree']\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def random_forest_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Vous pouvez également définir d'autres métriques, par exemple :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "\n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,         # Vous pouvez augmenter le nombre de folds (k-fold cross validation)\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.425e+01, tolerance: 6.630e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.576e+01, tolerance: 6.630e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 23\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 95\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'mean',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'max_depth': 5,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 200},\n",
       " 0.6048333738348641)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'n_estimators': [100, 200], # Nombre d'arbres\n",
    "    'max_depth': [None, 5, 10], # Profondeur maximale de l'arbre\n",
    "    'min_samples_split': [2, 5], # Nombre minimum d'échantillons pour diviser un nœud\n",
    "    'min_samples_leaf': [1, 2] # Nombre minimum d'échantillons requis à chaque feuille\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, random_forest_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.667\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Regression Logistique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def logistic_regression_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    # Note : pour utiliser la pénalisation L1, vous devez choisir un solver qui la supporte\n",
    "    # comme 'liblinear' ou 'saga'.\n",
    "    log_reg_model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Exemples d'autres métriques possibles :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "    # accuracy_scorer = 'accuracy'\n",
    "    \n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=log_reg_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,  # Vous pouvez mettre 'accuracy', recall_scorer, etc.\n",
    "        cv=3,               # Nombre de folds pour la cross-validation\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.425e+01, tolerance: 6.630e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.576e+01, tolerance: 6.630e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 23\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 96\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 95\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 24\n",
      "X_train from 2010-01-13 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'ffill',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'C': 0.01,\n",
       "  'penalty': 'l1',\n",
       "  'solver': 'saga'},\n",
       " 0.6237775752843818)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, logistic_regression_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.68\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "log_reg_model = LogisticRegression(random_state=42, penalty=best_params['penalty'], C=best_params['C'], solver=best_params['solver'], max_iter=10000)\n",
    "\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install tensorflow-macos\n",
    "# !pip install tensorflow-metal\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 95\n",
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "nan_strategy = 'ffill'\n",
    "lasso_alpha = 0.01\n",
    "\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']\n",
    "\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year='2014', train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Créer le modèle\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')  # pour classification binaire\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    # metrics accuracy, recall, precision, f1-score\n",
    "    metrics=[keras.metrics.Recall(name='recall'), keras.metrics.BinaryAccuracy(name='accuracy'), keras.metrics.Precision(name='precision')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.4859 - loss: 34.5302 - precision: 0.5010 - recall: 0.5356 - val_accuracy: 0.4444 - val_loss: 12.7325 - val_precision: 0.6667 - val_recall: 0.0822\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5251 - loss: 48.3800 - precision: 0.5655 - recall: 0.4505 - val_accuracy: 0.5238 - val_loss: 5.3796 - val_precision: 0.5915 - val_recall: 0.5753\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4913 - loss: 57.2961 - precision: 0.5300 - recall: 0.5285 - val_accuracy: 0.4167 - val_loss: 14.4663 - val_precision: 0.4000 - val_recall: 0.0137\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5043 - loss: 64.9395 - precision: 0.5108 - recall: 0.5017 - val_accuracy: 0.5873 - val_loss: 6.2398 - val_precision: 0.5955 - val_recall: 0.8973\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4936 - loss: 59.8823 - precision: 0.5250 - recall: 0.4948 - val_accuracy: 0.5833 - val_loss: 6.4370 - val_precision: 0.5945 - val_recall: 0.8836\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4955 - loss: 62.2621 - precision: 0.5121 - recall: 0.4994 - val_accuracy: 0.5873 - val_loss: 14.9232 - val_precision: 0.5854 - val_recall: 0.9863\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5149 - loss: 70.4051 - precision: 0.5303 - recall: 0.5643 - val_accuracy: 0.4206 - val_loss: 11.7561 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4929 - loss: 66.8665 - precision: 0.5115 - recall: 0.4545 - val_accuracy: 0.5794 - val_loss: 20.4477 - val_precision: 0.5794 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4971 - loss: 63.2668 - precision: 0.5192 - recall: 0.5616 - val_accuracy: 0.5794 - val_loss: 3.9987 - val_precision: 0.5962 - val_recall: 0.8493\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.5169 - loss: 60.9385 - precision: 0.5449 - recall: 0.5427 - val_accuracy: 0.5635 - val_loss: 3.3032 - val_precision: 0.5928 - val_recall: 0.7877\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1 Score: 0.676\n"
     ]
    }
   ],
   "source": [
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
