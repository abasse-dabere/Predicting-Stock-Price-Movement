{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2019-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2516, 55)\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "azn_df = pd.read_csv('../data/merged_data/AZN.csv')\n",
    "azn_df = azn_df[(azn_df['Date'] >= start_date) & (azn_df['Date'] <= end_date)]\n",
    "\n",
    "# Convertir la colonne 'Date' en datetime et la définir comme index\n",
    "azn_df['Date'] = pd.to_datetime(azn_df['Date'])\n",
    "azn_df.set_index('Date', inplace=True)\n",
    "\n",
    "print(azn_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>news_neg</th>\n",
       "      <th>news_neu</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.978907</td>\n",
       "      <td>0.975040</td>\n",
       "      <td>0.972611</td>\n",
       "      <td>0.963860</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.905915</td>\n",
       "      <td>0.982968</td>\n",
       "      <td>0.980982</td>\n",
       "      <td>0.979311</td>\n",
       "      <td>0.972745</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.604342</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.239686</td>\n",
       "      <td>0.706169</td>\n",
       "      <td>0.054145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>1.001098</td>\n",
       "      <td>0.996297</td>\n",
       "      <td>0.992874</td>\n",
       "      <td>0.983974</td>\n",
       "      <td>0.979251</td>\n",
       "      <td>0.926098</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>1.001063</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>0.993379</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.311568</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.142433</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.052957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>1.010590</td>\n",
       "      <td>1.004731</td>\n",
       "      <td>1.000792</td>\n",
       "      <td>0.992266</td>\n",
       "      <td>0.986986</td>\n",
       "      <td>0.934902</td>\n",
       "      <td>1.008786</td>\n",
       "      <td>1.007701</td>\n",
       "      <td>1.006619</td>\n",
       "      <td>1.001255</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.199394</td>\n",
       "      <td>0.730939</td>\n",
       "      <td>0.069668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>1.000837</td>\n",
       "      <td>0.994633</td>\n",
       "      <td>0.990994</td>\n",
       "      <td>0.982151</td>\n",
       "      <td>0.976520</td>\n",
       "      <td>0.926291</td>\n",
       "      <td>0.998328</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.996372</td>\n",
       "      <td>0.991210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026727</td>\n",
       "      <td>0.919697</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.123578</td>\n",
       "      <td>0.822368</td>\n",
       "      <td>0.054053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.998290</td>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.988286</td>\n",
       "      <td>0.978204</td>\n",
       "      <td>0.972674</td>\n",
       "      <td>0.923738</td>\n",
       "      <td>0.995140</td>\n",
       "      <td>0.994161</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>0.987938</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288173</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.804381</td>\n",
       "      <td>0.083562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-04      0.978907      0.975040      0.972611      0.963860   \n",
       "2010-01-05      1.001098      0.996297      0.992874      0.983974   \n",
       "2010-01-06      1.010590      1.004731      1.000792      0.992266   \n",
       "2010-01-07      1.000837      0.994633      0.990994      0.982151   \n",
       "2010-01-08      0.998290      0.991278      0.988286      0.978204   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-04       0.959357       0.905915      0.982968      0.980982   \n",
       "2010-01-05       0.979251       0.926098      1.002687      1.001063   \n",
       "2010-01-06       0.986986       0.934902      1.008786      1.007701   \n",
       "2010-01-07       0.976520       0.926291      0.998328      0.997363   \n",
       "2010-01-08       0.972674       0.923738      0.995140      0.994161   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  news_neg  news_neu  news_pos  \\\n",
       "Date                                    ...                                 \n",
       "2010-01-04      0.979311      0.972745  ...       NaN       NaN       NaN   \n",
       "2010-01-05      0.999611      0.993379  ...       NaN       NaN       NaN   \n",
       "2010-01-06      1.006619      1.001255  ...       NaN       NaN       NaN   \n",
       "2010-01-07      0.996372      0.991210  ...  0.026727  0.919697  0.053576   \n",
       "2010-01-08      0.993164      0.987938  ...       NaN       NaN       NaN   \n",
       "\n",
       "            sp500_return_pct  gold_return_pct  vix_close  bond_yields_close  \\\n",
       "Date                                                                          \n",
       "2010-01-04          1.604342         2.054419  20.040001              0.055   \n",
       "2010-01-05          0.311568         0.035790  19.350000              0.060   \n",
       "2010-01-06          0.054552         1.591991  19.160000              0.045   \n",
       "2010-01-07          0.400120        -0.246505  19.059999              0.045   \n",
       "2010-01-08          0.288173         0.450091  18.129999              0.040   \n",
       "\n",
       "            sector_reddit_neg  sector_reddit_neu  sector_reddit_pos  \n",
       "Date                                                                 \n",
       "2010-01-04           0.239686           0.706169           0.054145  \n",
       "2010-01-05           0.142433           0.804610           0.052957  \n",
       "2010-01-06           0.199394           0.730939           0.069668  \n",
       "2010-01-07           0.123578           0.822368           0.054053  \n",
       "2010-01-08           0.112057           0.804381           0.083562  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les premières lignes\n",
    "azn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\n",
    "    'stock_SMA_10', 'stock_SMA_15', 'stock_SMA_20', 'stock_SMA_50',\n",
    "    'stock_SMA_100', 'stock_SMA_200', 'stock_EMA_10', 'stock_EMA_12',\n",
    "    'stock_EMA_14', 'stock_EMA_26', 'stock_EMA_30', 'stock_EMA_50',\n",
    "    'stock_EMA_100', 'stock_ADX_14', 'stock_ADX_14_neg', 'stock_ADX_14_pos',\n",
    "    'stock_ADX_20', 'stock_ADX_20_neg', 'stock_ADX_20_pos', 'stock_ADX_25',\n",
    "    'stock_ADX_25_neg', 'stock_ADX_25_pos', 'stock_ADX_30',\n",
    "    'stock_ADX_30_neg', 'stock_ADX_30_pos', 'stock_ATR_14', 'stock_ATR_20',\n",
    "    'stock_ATR_28', 'stock_RSI_7', 'stock_RSI_14', 'stock_RSI_21',\n",
    "    'stock_Stoch_14', 'stock_Stoch_14_signal', 'stock_Stoch_21',\n",
    "    'stock_Stoch_21_signal', 'stock_Stoch_28', 'stock_Stoch_28_signal',\n",
    "    'stock_CMF_14', 'stock_CMF_20', 'stock_CMF_28', 'stock_VROC_7',\n",
    "    'stock_VROC_14', 'stock_VROC_21', 'stock_VROC_28'\n",
    "]\n",
    "\n",
    "print(len(INDICATORS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>stock_Stoch_21_signal</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_Stoch_28_signal</th>\n",
       "      <th>stock_CMF_14</th>\n",
       "      <th>stock_CMF_20</th>\n",
       "      <th>stock_CMF_28</th>\n",
       "      <th>stock_VROC_7</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>stock_VROC_21</th>\n",
       "      <th>stock_VROC_28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.997974</td>\n",
       "      <td>0.994596</td>\n",
       "      <td>0.988776</td>\n",
       "      <td>0.977424</td>\n",
       "      <td>0.999050</td>\n",
       "      <td>0.998834</td>\n",
       "      <td>0.998616</td>\n",
       "      <td>0.997285</td>\n",
       "      <td>...</td>\n",
       "      <td>54.382151</td>\n",
       "      <td>54.715641</td>\n",
       "      <td>54.722912</td>\n",
       "      <td>0.067789</td>\n",
       "      <td>0.068105</td>\n",
       "      <td>0.067710</td>\n",
       "      <td>0.215397</td>\n",
       "      <td>0.239952</td>\n",
       "      <td>0.270762</td>\n",
       "      <td>0.281004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.028567</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.049146</td>\n",
       "      <td>0.060806</td>\n",
       "      <td>0.072510</td>\n",
       "      <td>0.019234</td>\n",
       "      <td>0.021305</td>\n",
       "      <td>0.023145</td>\n",
       "      <td>0.031321</td>\n",
       "      <td>...</td>\n",
       "      <td>29.439334</td>\n",
       "      <td>30.725798</td>\n",
       "      <td>29.539710</td>\n",
       "      <td>0.202878</td>\n",
       "      <td>0.176060</td>\n",
       "      <td>0.151061</td>\n",
       "      <td>0.934657</td>\n",
       "      <td>0.972575</td>\n",
       "      <td>1.251976</td>\n",
       "      <td>1.134685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.870549</td>\n",
       "      <td>0.850002</td>\n",
       "      <td>0.835720</td>\n",
       "      <td>0.824018</td>\n",
       "      <td>0.790804</td>\n",
       "      <td>0.714095</td>\n",
       "      <td>0.889232</td>\n",
       "      <td>0.882422</td>\n",
       "      <td>0.877292</td>\n",
       "      <td>0.851702</td>\n",
       "      <td>...</td>\n",
       "      <td>1.563984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.335420</td>\n",
       "      <td>-0.599236</td>\n",
       "      <td>-0.462590</td>\n",
       "      <td>-0.360398</td>\n",
       "      <td>-0.882247</td>\n",
       "      <td>-0.928983</td>\n",
       "      <td>-0.924494</td>\n",
       "      <td>-0.960206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.985978</td>\n",
       "      <td>0.981741</td>\n",
       "      <td>0.977984</td>\n",
       "      <td>0.962577</td>\n",
       "      <td>0.946722</td>\n",
       "      <td>0.926772</td>\n",
       "      <td>0.987935</td>\n",
       "      <td>0.986333</td>\n",
       "      <td>0.985285</td>\n",
       "      <td>0.977854</td>\n",
       "      <td>...</td>\n",
       "      <td>27.285021</td>\n",
       "      <td>27.550227</td>\n",
       "      <td>27.450265</td>\n",
       "      <td>-0.073103</td>\n",
       "      <td>-0.050336</td>\n",
       "      <td>-0.035473</td>\n",
       "      <td>-0.321027</td>\n",
       "      <td>-0.340903</td>\n",
       "      <td>-0.326953</td>\n",
       "      <td>-0.344423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.998442</td>\n",
       "      <td>0.997662</td>\n",
       "      <td>0.997573</td>\n",
       "      <td>0.992250</td>\n",
       "      <td>0.985899</td>\n",
       "      <td>0.978686</td>\n",
       "      <td>0.998412</td>\n",
       "      <td>0.998294</td>\n",
       "      <td>0.998036</td>\n",
       "      <td>0.996223</td>\n",
       "      <td>...</td>\n",
       "      <td>56.928034</td>\n",
       "      <td>57.190275</td>\n",
       "      <td>57.440262</td>\n",
       "      <td>0.070946</td>\n",
       "      <td>0.070132</td>\n",
       "      <td>0.070644</td>\n",
       "      <td>-0.016184</td>\n",
       "      <td>-0.008046</td>\n",
       "      <td>-0.003828</td>\n",
       "      <td>-0.006860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.011809</td>\n",
       "      <td>1.015567</td>\n",
       "      <td>1.018049</td>\n",
       "      <td>1.025153</td>\n",
       "      <td>1.028889</td>\n",
       "      <td>1.027705</td>\n",
       "      <td>1.009969</td>\n",
       "      <td>1.011352</td>\n",
       "      <td>1.012370</td>\n",
       "      <td>1.016586</td>\n",
       "      <td>...</td>\n",
       "      <td>82.334888</td>\n",
       "      <td>83.071351</td>\n",
       "      <td>82.566648</td>\n",
       "      <td>0.206306</td>\n",
       "      <td>0.185469</td>\n",
       "      <td>0.165856</td>\n",
       "      <td>0.469359</td>\n",
       "      <td>0.508191</td>\n",
       "      <td>0.488911</td>\n",
       "      <td>0.527818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.152181</td>\n",
       "      <td>1.151154</td>\n",
       "      <td>1.159795</td>\n",
       "      <td>1.198589</td>\n",
       "      <td>1.212743</td>\n",
       "      <td>1.189087</td>\n",
       "      <td>1.138325</td>\n",
       "      <td>1.143009</td>\n",
       "      <td>1.146705</td>\n",
       "      <td>1.158279</td>\n",
       "      <td>...</td>\n",
       "      <td>99.098891</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.258850</td>\n",
       "      <td>0.791610</td>\n",
       "      <td>0.703882</td>\n",
       "      <td>0.590226</td>\n",
       "      <td>12.975607</td>\n",
       "      <td>11.165660</td>\n",
       "      <td>26.790792</td>\n",
       "      <td>13.013314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  stock_SMA_100  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.999055      0.998515      0.997974      0.994596       0.988776   \n",
       "std        0.023031      0.028567      0.032912      0.049146       0.060806   \n",
       "min        0.870549      0.850002      0.835720      0.824018       0.790804   \n",
       "25%        0.985978      0.981741      0.977984      0.962577       0.946722   \n",
       "50%        0.998442      0.997662      0.997573      0.992250       0.985899   \n",
       "75%        1.011809      1.015567      1.018049      1.025153       1.028889   \n",
       "max        1.152181      1.151154      1.159795      1.198589       1.212743   \n",
       "\n",
       "       stock_SMA_200  stock_EMA_10  stock_EMA_12  stock_EMA_14  stock_EMA_26  \\\n",
       "count    2516.000000   2516.000000   2516.000000   2516.000000   2516.000000   \n",
       "mean        0.977424      0.999050      0.998834      0.998616      0.997285   \n",
       "std         0.072510      0.019234      0.021305      0.023145      0.031321   \n",
       "min         0.714095      0.889232      0.882422      0.877292      0.851702   \n",
       "25%         0.926772      0.987935      0.986333      0.985285      0.977854   \n",
       "50%         0.978686      0.998412      0.998294      0.998036      0.996223   \n",
       "75%         1.027705      1.009969      1.011352      1.012370      1.016586   \n",
       "max         1.189087      1.138325      1.143009      1.146705      1.158279   \n",
       "\n",
       "       ...  stock_Stoch_21_signal  stock_Stoch_28  stock_Stoch_28_signal  \\\n",
       "count  ...            2516.000000     2516.000000            2516.000000   \n",
       "mean   ...              54.382151       54.715641              54.722912   \n",
       "std    ...              29.439334       30.725798              29.539710   \n",
       "min    ...               1.563984        0.000000               1.335420   \n",
       "25%    ...              27.285021       27.550227              27.450265   \n",
       "50%    ...              56.928034       57.190275              57.440262   \n",
       "75%    ...              82.334888       83.071351              82.566648   \n",
       "max    ...              99.098891      100.000000              99.258850   \n",
       "\n",
       "       stock_CMF_14  stock_CMF_20  stock_CMF_28  stock_VROC_7  stock_VROC_14  \\\n",
       "count   2516.000000   2516.000000   2516.000000   2516.000000    2516.000000   \n",
       "mean       0.067789      0.068105      0.067710      0.215397       0.239952   \n",
       "std        0.202878      0.176060      0.151061      0.934657       0.972575   \n",
       "min       -0.599236     -0.462590     -0.360398     -0.882247      -0.928983   \n",
       "25%       -0.073103     -0.050336     -0.035473     -0.321027      -0.340903   \n",
       "50%        0.070946      0.070132      0.070644     -0.016184      -0.008046   \n",
       "75%        0.206306      0.185469      0.165856      0.469359       0.508191   \n",
       "max        0.791610      0.703882      0.590226     12.975607      11.165660   \n",
       "\n",
       "       stock_VROC_21  stock_VROC_28  \n",
       "count    2516.000000    2516.000000  \n",
       "mean        0.270762       0.281004  \n",
       "std         1.251976       1.134685  \n",
       "min        -0.924494      -0.960206  \n",
       "25%        -0.326953      -0.344423  \n",
       "50%        -0.003828      -0.006860  \n",
       "75%         0.488911       0.527818  \n",
       "max        26.790792      13.013314  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des indicateurs\n",
    "azn_df[INDICATORS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_target</th>\n",
       "      <th>news_neg</th>\n",
       "      <th>news_neu</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>sp500_return_pct</th>\n",
       "      <th>gold_return_pct</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2516.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2512.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "      <td>2516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.040025</td>\n",
       "      <td>0.069771</td>\n",
       "      <td>0.868806</td>\n",
       "      <td>0.061423</td>\n",
       "      <td>0.046836</td>\n",
       "      <td>0.017741</td>\n",
       "      <td>16.861692</td>\n",
       "      <td>0.554658</td>\n",
       "      <td>0.131712</td>\n",
       "      <td>0.782792</td>\n",
       "      <td>0.085258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.416213</td>\n",
       "      <td>0.107181</td>\n",
       "      <td>0.124693</td>\n",
       "      <td>0.076547</td>\n",
       "      <td>0.931054</td>\n",
       "      <td>0.995375</td>\n",
       "      <td>5.634105</td>\n",
       "      <td>0.776166</td>\n",
       "      <td>0.063496</td>\n",
       "      <td>0.068973</td>\n",
       "      <td>0.038096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-14.908661</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.029431</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>-6.663446</td>\n",
       "      <td>-9.353766</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.025058</td>\n",
       "      <td>0.462875</td>\n",
       "      <td>0.030920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.660797</td>\n",
       "      <td>0.027850</td>\n",
       "      <td>0.882078</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>-0.326374</td>\n",
       "      <td>-0.456111</td>\n",
       "      <td>13.040000</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.083071</td>\n",
       "      <td>0.739104</td>\n",
       "      <td>0.057807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.040877</td>\n",
       "      <td>0.042738</td>\n",
       "      <td>0.908122</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.060024</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>15.475000</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>0.791866</td>\n",
       "      <td>0.073022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.752142</td>\n",
       "      <td>0.065433</td>\n",
       "      <td>0.919767</td>\n",
       "      <td>0.056999</td>\n",
       "      <td>0.505720</td>\n",
       "      <td>0.535977</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.170053</td>\n",
       "      <td>0.833663</td>\n",
       "      <td>0.103731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12.161372</td>\n",
       "      <td>0.950888</td>\n",
       "      <td>0.945476</td>\n",
       "      <td>0.882347</td>\n",
       "      <td>4.959374</td>\n",
       "      <td>4.710198</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.408000</td>\n",
       "      <td>0.403484</td>\n",
       "      <td>0.921713</td>\n",
       "      <td>0.275290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_target    news_neg    news_neu    news_pos  sp500_return_pct  \\\n",
       "count   2516.000000  739.000000  739.000000  739.000000       2512.000000   \n",
       "mean       0.040025    0.069771    0.868806    0.061423          0.046836   \n",
       "std        1.416213    0.107181    0.124693    0.076547          0.931054   \n",
       "min      -14.908661    0.010037    0.029431    0.012495         -6.663446   \n",
       "25%       -0.660797    0.027850    0.882078    0.034401         -0.326374   \n",
       "50%        0.040877    0.042738    0.908122    0.042641          0.060024   \n",
       "75%        0.752142    0.065433    0.919767    0.056999          0.505720   \n",
       "max       12.161372    0.950888    0.945476    0.882347          4.959374   \n",
       "\n",
       "       gold_return_pct    vix_close  bond_yields_close  sector_reddit_neg  \\\n",
       "count      2512.000000  2512.000000        2512.000000        2516.000000   \n",
       "mean          0.017741    16.861692           0.554658           0.131712   \n",
       "std           0.995375     5.634105           0.776166           0.063496   \n",
       "min          -9.353766     9.140000           0.003000           0.025058   \n",
       "25%          -0.456111    13.040000           0.035000           0.083071   \n",
       "50%           0.018708    15.475000           0.117500           0.122378   \n",
       "75%           0.535977    18.900000           0.985000           0.170053   \n",
       "max           4.710198    48.000000           2.408000           0.403484   \n",
       "\n",
       "       sector_reddit_neu  sector_reddit_pos  \n",
       "count        2516.000000        2516.000000  \n",
       "mean            0.782792           0.085258  \n",
       "std             0.068973           0.038096  \n",
       "min             0.462875           0.030920  \n",
       "25%             0.739104           0.057807  \n",
       "50%             0.791866           0.073022  \n",
       "75%             0.833663           0.103731  \n",
       "max             0.921713           0.275290  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher les statistiques descriptives des autres colonnes\n",
    "azn_df.drop(INDICATORS, axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion des Valeurs Manquantes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant imputation:\n",
      " news_neg             1777\n",
      "news_neu             1777\n",
      "news_pos             1777\n",
      "sp500_return_pct        4\n",
      "gold_return_pct         4\n",
      "vix_close               4\n",
      "bond_yields_close       4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print le nombre de valeurs manquantes\n",
    "missing_values = azn_df.isnull().sum()\n",
    "print(\"Valeurs manquantes avant imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la médiane\n",
    "azn_df_imputed_median = azn_df.fillna(azn_df.median())\n",
    "\n",
    "missing_values = azn_df_imputed_median.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec la moyenne\n",
    "azn_df_imputed_mean = azn_df.fillna(azn_df.mean())\n",
    "\n",
    "missing_values = azn_df_imputed_mean.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " news_neg    3\n",
      "news_neu    3\n",
      "news_pos    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec une interpolation linéaire\n",
    "azn_df_imputed_interpolate = azn_df.interpolate(method='linear')\n",
    "\n",
    "missing_values = azn_df_imputed_interpolate.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation:\n",
      " news_neg    3\n",
      "news_neu    3\n",
      "news_pos    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputation avec forward fill\n",
    "azn_df_imputed_ffill = azn_df.ffill()\n",
    "\n",
    "missing_values = azn_df_imputed_ffill.isnull().sum()\n",
    "print(\"Valeurs manquantes après imputation:\\n\", missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, method='ffill'):\n",
    "    if method == 'median':\n",
    "        return df.fillna(df.median())\n",
    "    elif method == 'mean':\n",
    "        return df.fillna(df.mean())\n",
    "    elif method == 'interpolate':\n",
    "        return df.interpolate(method='linear')\n",
    "    elif method == 'ffill':\n",
    "        return df.ffill()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de la Variable Cible (stock_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target\n",
      "1    1294\n",
      "0    1222\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target = azn_df['stock_target']\n",
    "target = target.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "target_counts = target.value_counts()\n",
    "print(target_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des Données pour le Modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créer des variables lags pour les indicateurs\n",
    "def create_lag_variables(data, features, lags=[1, 2, 3, 4, 5, 6, 7]):\n",
    "    df = data.copy()\n",
    "    lagged_columns = {}\n",
    "\n",
    "    for feature in features:\n",
    "        for lag in lags:\n",
    "            lagged_columns[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "    \n",
    "    lagged_df = pd.DataFrame(lagged_columns, index=df.index)\n",
    "    df = pd.concat([df, lagged_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "azn_df_lagged = create_lag_variables(azn_df, azn_df.columns)\n",
    "azn_df_lagged = impute_missing_values(azn_df_lagged, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_target              1.000000\n",
      "stock_EMA_100             0.056944\n",
      "stock_SMA_100             0.056583\n",
      "stock_SMA_200             0.056017\n",
      "stock_EMA_100_lag_6       0.053926\n",
      "                            ...   \n",
      "stock_ADX_20_pos_lag_7   -0.051905\n",
      "stock_ADX_30_pos_lag_6   -0.052081\n",
      "stock_ADX_25_pos_lag_7   -0.052661\n",
      "stock_ADX_30_pos_lag_7   -0.053018\n",
      "stock_VROC_21_lag_5      -0.056531\n",
      "Name: stock_target, Length: 440, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculer la corrélation entre les features et la cible\n",
    "correlations = azn_df_lagged.corr()['stock_target'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516, 440)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azn_df_lagged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection avec Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = azn_df_lagged.copy()\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['stock_target'], axis=1)\n",
    "y = data['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardiser les features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>stock_EMA_26</th>\n",
       "      <th>...</th>\n",
       "      <th>sector_reddit_neu_lag_5</th>\n",
       "      <th>sector_reddit_neu_lag_6</th>\n",
       "      <th>sector_reddit_neu_lag_7</th>\n",
       "      <th>sector_reddit_pos_lag_1</th>\n",
       "      <th>sector_reddit_pos_lag_2</th>\n",
       "      <th>sector_reddit_pos_lag_3</th>\n",
       "      <th>sector_reddit_pos_lag_4</th>\n",
       "      <th>sector_reddit_pos_lag_5</th>\n",
       "      <th>sector_reddit_pos_lag_6</th>\n",
       "      <th>sector_reddit_pos_lag_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>-2.195365</td>\n",
       "      <td>-1.950403</td>\n",
       "      <td>-1.924870</td>\n",
       "      <td>-1.580449</td>\n",
       "      <td>-1.362630</td>\n",
       "      <td>-1.542106</td>\n",
       "      <td>-2.145687</td>\n",
       "      <td>-2.126951</td>\n",
       "      <td>-2.103316</td>\n",
       "      <td>-1.956262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511895</td>\n",
       "      <td>0.311903</td>\n",
       "      <td>0.572709</td>\n",
       "      <td>3.241149</td>\n",
       "      <td>0.165537</td>\n",
       "      <td>1.424094</td>\n",
       "      <td>0.092658</td>\n",
       "      <td>-0.299126</td>\n",
       "      <td>-0.044918</td>\n",
       "      <td>-0.819254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-20</th>\n",
       "      <td>-1.699615</td>\n",
       "      <td>-1.652951</td>\n",
       "      <td>-1.665400</td>\n",
       "      <td>-1.459824</td>\n",
       "      <td>-1.287527</td>\n",
       "      <td>-1.471189</td>\n",
       "      <td>-1.576556</td>\n",
       "      <td>-1.633247</td>\n",
       "      <td>-1.666429</td>\n",
       "      <td>-1.689386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996398</td>\n",
       "      <td>0.511418</td>\n",
       "      <td>0.312010</td>\n",
       "      <td>-0.920531</td>\n",
       "      <td>3.241023</td>\n",
       "      <td>0.164723</td>\n",
       "      <td>1.426032</td>\n",
       "      <td>0.092479</td>\n",
       "      <td>-0.298729</td>\n",
       "      <td>-0.044763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-21</th>\n",
       "      <td>-0.841983</td>\n",
       "      <td>-1.072335</td>\n",
       "      <td>-1.157356</td>\n",
       "      <td>-1.172429</td>\n",
       "      <td>-1.079119</td>\n",
       "      <td>-1.291060</td>\n",
       "      <td>-0.732269</td>\n",
       "      <td>-0.863773</td>\n",
       "      <td>-0.957632</td>\n",
       "      <td>-1.186117</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.236832</td>\n",
       "      <td>0.996069</td>\n",
       "      <td>0.511537</td>\n",
       "      <td>0.640417</td>\n",
       "      <td>-0.920826</td>\n",
       "      <td>3.239096</td>\n",
       "      <td>0.165630</td>\n",
       "      <td>1.425988</td>\n",
       "      <td>0.092980</td>\n",
       "      <td>-0.298549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-22</th>\n",
       "      <td>-0.040290</td>\n",
       "      <td>-0.505973</td>\n",
       "      <td>-0.659335</td>\n",
       "      <td>-0.879126</td>\n",
       "      <td>-0.856308</td>\n",
       "      <td>-1.098208</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>-0.172758</td>\n",
       "      <td>-0.305817</td>\n",
       "      <td>-0.691147</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.225332</td>\n",
       "      <td>-1.237843</td>\n",
       "      <td>0.996217</td>\n",
       "      <td>1.829658</td>\n",
       "      <td>0.640185</td>\n",
       "      <td>-0.921247</td>\n",
       "      <td>3.242521</td>\n",
       "      <td>0.165458</td>\n",
       "      <td>1.426839</td>\n",
       "      <td>0.093122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-25</th>\n",
       "      <td>-0.348232</td>\n",
       "      <td>-0.818773</td>\n",
       "      <td>-0.923893</td>\n",
       "      <td>-1.092437</td>\n",
       "      <td>-1.039818</td>\n",
       "      <td>-1.232606</td>\n",
       "      <td>-0.523110</td>\n",
       "      <td>-0.633565</td>\n",
       "      <td>-0.722777</td>\n",
       "      <td>-0.995175</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.917005</td>\n",
       "      <td>-1.226339</td>\n",
       "      <td>-1.237829</td>\n",
       "      <td>2.734590</td>\n",
       "      <td>1.829474</td>\n",
       "      <td>0.639199</td>\n",
       "      <td>-0.921229</td>\n",
       "      <td>3.242660</td>\n",
       "      <td>0.165978</td>\n",
       "      <td>1.426854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 439 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "Date                                                                 \n",
       "2010-01-19     -2.195365     -1.950403     -1.924870     -1.580449   \n",
       "2010-01-20     -1.699615     -1.652951     -1.665400     -1.459824   \n",
       "2010-01-21     -0.841983     -1.072335     -1.157356     -1.172429   \n",
       "2010-01-22     -0.040290     -0.505973     -0.659335     -0.879126   \n",
       "2010-01-25     -0.348232     -0.818773     -0.923893     -1.092437   \n",
       "\n",
       "            stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  \\\n",
       "Date                                                                   \n",
       "2010-01-19      -1.362630      -1.542106     -2.145687     -2.126951   \n",
       "2010-01-20      -1.287527      -1.471189     -1.576556     -1.633247   \n",
       "2010-01-21      -1.079119      -1.291060     -0.732269     -0.863773   \n",
       "2010-01-22      -0.856308      -1.098208      0.001338     -0.172758   \n",
       "2010-01-25      -1.039818      -1.232606     -0.523110     -0.633565   \n",
       "\n",
       "            stock_EMA_14  stock_EMA_26  ...  sector_reddit_neu_lag_5  \\\n",
       "Date                                    ...                            \n",
       "2010-01-19     -2.103316     -1.956262  ...                 0.511895   \n",
       "2010-01-20     -1.666429     -1.689386  ...                 0.996398   \n",
       "2010-01-21     -0.957632     -1.186117  ...                -1.236832   \n",
       "2010-01-22     -0.305817     -0.691147  ...                -1.225332   \n",
       "2010-01-25     -0.722777     -0.995175  ...                -1.917005   \n",
       "\n",
       "            sector_reddit_neu_lag_6  sector_reddit_neu_lag_7  \\\n",
       "Date                                                           \n",
       "2010-01-19                 0.311903                 0.572709   \n",
       "2010-01-20                 0.511418                 0.312010   \n",
       "2010-01-21                 0.996069                 0.511537   \n",
       "2010-01-22                -1.237843                 0.996217   \n",
       "2010-01-25                -1.226339                -1.237829   \n",
       "\n",
       "            sector_reddit_pos_lag_1  sector_reddit_pos_lag_2  \\\n",
       "Date                                                           \n",
       "2010-01-19                 3.241149                 0.165537   \n",
       "2010-01-20                -0.920531                 3.241023   \n",
       "2010-01-21                 0.640417                -0.920826   \n",
       "2010-01-22                 1.829658                 0.640185   \n",
       "2010-01-25                 2.734590                 1.829474   \n",
       "\n",
       "            sector_reddit_pos_lag_3  sector_reddit_pos_lag_4  \\\n",
       "Date                                                           \n",
       "2010-01-19                 1.424094                 0.092658   \n",
       "2010-01-20                 0.164723                 1.426032   \n",
       "2010-01-21                 3.239096                 0.165630   \n",
       "2010-01-22                -0.921247                 3.242521   \n",
       "2010-01-25                 0.639199                -0.921229   \n",
       "\n",
       "            sector_reddit_pos_lag_5  sector_reddit_pos_lag_6  \\\n",
       "Date                                                           \n",
       "2010-01-19                -0.299126                -0.044918   \n",
       "2010-01-20                 0.092479                -0.298729   \n",
       "2010-01-21                 1.425988                 0.092980   \n",
       "2010-01-22                 0.165458                 1.426839   \n",
       "2010-01-25                 3.242660                 0.165978   \n",
       "\n",
       "            sector_reddit_pos_lag_7  \n",
       "Date                                 \n",
       "2010-01-19                -0.819254  \n",
       "2010-01-20                -0.044763  \n",
       "2010-01-21                -0.298549  \n",
       "2010-01-22                 0.093122  \n",
       "2010-01-25                 1.426854  \n",
       "\n",
       "[5 rows x 439 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer LASO pour la sélection des features\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 98\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=0.01, max_iter=10000)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# nombre de features sélectionnées\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(f'Nombre de features sélectionnées: {len(selected_features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indic_coef = dict(zip(X.columns, lasso.coef_))\n",
    "non_zero_coef = {k: v for k, v in indic_coef.items() if v != 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_zero_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les features sélectionnées\n",
    "azn_df_selected = data[selected_features.append(pd.Index(['stock_target']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_RSI_7</th>\n",
       "      <th>stock_RSI_14</th>\n",
       "      <th>stock_Stoch_14</th>\n",
       "      <th>stock_Stoch_21</th>\n",
       "      <th>stock_Stoch_28</th>\n",
       "      <th>stock_VROC_14</th>\n",
       "      <th>stock_VROC_28</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>stock_ADX_14_lag_1</th>\n",
       "      <th>stock_ADX_14_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>gold_return_pct_lag_5</th>\n",
       "      <th>gold_return_pct_lag_6</th>\n",
       "      <th>gold_return_pct_lag_7</th>\n",
       "      <th>vix_close_lag_2</th>\n",
       "      <th>vix_close_lag_3</th>\n",
       "      <th>vix_close_lag_4</th>\n",
       "      <th>vix_close_lag_5</th>\n",
       "      <th>vix_close_lag_6</th>\n",
       "      <th>bond_yields_close_lag_7</th>\n",
       "      <th>stock_target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>78.908876</td>\n",
       "      <td>72.616719</td>\n",
       "      <td>97.773344</td>\n",
       "      <td>97.920664</td>\n",
       "      <td>97.966787</td>\n",
       "      <td>3.446149</td>\n",
       "      <td>1.673185</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>15.237649</td>\n",
       "      <td>12.329139</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.416672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-20</th>\n",
       "      <td>75.159694</td>\n",
       "      <td>70.562408</td>\n",
       "      <td>93.522272</td>\n",
       "      <td>93.950853</td>\n",
       "      <td>94.085033</td>\n",
       "      <td>3.284679</td>\n",
       "      <td>0.266107</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>17.052429</td>\n",
       "      <td>14.247458</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.894493</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-1.315003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-21</th>\n",
       "      <td>64.008668</td>\n",
       "      <td>64.396527</td>\n",
       "      <td>77.192949</td>\n",
       "      <td>78.007480</td>\n",
       "      <td>78.649597</td>\n",
       "      <td>2.555955</td>\n",
       "      <td>0.601946</td>\n",
       "      <td>22.270000</td>\n",
       "      <td>18.696738</td>\n",
       "      <td>15.237649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>-1.894493</td>\n",
       "      <td>1.098225</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-1.393092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-22</th>\n",
       "      <td>54.200569</td>\n",
       "      <td>58.628592</td>\n",
       "      <td>63.742686</td>\n",
       "      <td>63.742686</td>\n",
       "      <td>66.058380</td>\n",
       "      <td>1.148925</td>\n",
       "      <td>-0.283228</td>\n",
       "      <td>27.309999</td>\n",
       "      <td>20.487379</td>\n",
       "      <td>17.052429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545578</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>-1.894493</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.269449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-25</th>\n",
       "      <td>60.539275</td>\n",
       "      <td>61.928384</td>\n",
       "      <td>75.828432</td>\n",
       "      <td>75.828432</td>\n",
       "      <td>77.372230</td>\n",
       "      <td>0.030773</td>\n",
       "      <td>0.121576</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>21.176790</td>\n",
       "      <td>18.696738</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.093996</td>\n",
       "      <td>0.545578</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>22.270000</td>\n",
       "      <td>18.680000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>17.910000</td>\n",
       "      <td>17.629999</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.233321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            stock_RSI_7  stock_RSI_14  stock_Stoch_14  stock_Stoch_21  \\\n",
       "Date                                                                    \n",
       "2010-01-19    78.908876     72.616719       97.773344       97.920664   \n",
       "2010-01-20    75.159694     70.562408       93.522272       93.950853   \n",
       "2010-01-21    64.008668     64.396527       77.192949       78.007480   \n",
       "2010-01-22    54.200569     58.628592       63.742686       63.742686   \n",
       "2010-01-25    60.539275     61.928384       75.828432       75.828432   \n",
       "\n",
       "            stock_Stoch_28  stock_VROC_14  stock_VROC_28  vix_close  \\\n",
       "Date                                                                  \n",
       "2010-01-19       97.966787       3.446149       1.673185  17.580000   \n",
       "2010-01-20       94.085033       3.284679       0.266107  18.680000   \n",
       "2010-01-21       78.649597       2.555955       0.601946  22.270000   \n",
       "2010-01-22       66.058380       1.148925      -0.283228  27.309999   \n",
       "2010-01-25       77.372230       0.030773       0.121576  25.410000   \n",
       "\n",
       "            stock_ADX_14_lag_1  stock_ADX_14_lag_3  ...  \\\n",
       "Date                                                ...   \n",
       "2010-01-19           15.237649           12.329139  ...   \n",
       "2010-01-20           17.052429           14.247458  ...   \n",
       "2010-01-21           18.696738           15.237649  ...   \n",
       "2010-01-22           20.487379           17.052429  ...   \n",
       "2010-01-25           21.176790           18.696738  ...   \n",
       "\n",
       "            gold_return_pct_lag_5  gold_return_pct_lag_6  \\\n",
       "Date                                                       \n",
       "2010-01-19               1.098225               0.450091   \n",
       "2010-01-20              -1.894493               1.098225   \n",
       "2010-01-21               0.664364              -1.894493   \n",
       "2010-01-22               0.545578               0.664364   \n",
       "2010-01-25              -1.093996               0.545578   \n",
       "\n",
       "            gold_return_pct_lag_7  vix_close_lag_2  vix_close_lag_3  \\\n",
       "Date                                                                  \n",
       "2010-01-19              -0.246505        17.629999        17.850000   \n",
       "2010-01-20               0.450091        17.910000        17.629999   \n",
       "2010-01-21               1.098225        17.580000        17.910000   \n",
       "2010-01-22              -1.894493        18.680000        17.580000   \n",
       "2010-01-25               0.664364        22.270000        18.680000   \n",
       "\n",
       "            vix_close_lag_4  vix_close_lag_5  vix_close_lag_6  \\\n",
       "Date                                                            \n",
       "2010-01-19        18.250000        17.549999        18.129999   \n",
       "2010-01-20        17.850000        18.250000        17.549999   \n",
       "2010-01-21        17.629999        17.850000        18.250000   \n",
       "2010-01-22        17.910000        17.629999        17.850000   \n",
       "2010-01-25        17.580000        17.910000        17.629999   \n",
       "\n",
       "            bond_yields_close_lag_7  stock_target  \n",
       "Date                                               \n",
       "2010-01-19                    0.045     -0.416672  \n",
       "2010-01-20                    0.040     -1.315003  \n",
       "2010-01-21                    0.025     -1.393092  \n",
       "2010-01-22                    0.040      1.269449  \n",
       "2010-01-25                    0.050      1.233321  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azn_df_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: XGBOOST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = azn_df_selected.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==1.5.2 in /opt/anaconda3/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.5.2) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost\n",
    "# !pip uninstall -y scikit-learn\n",
    "!pip install \"scikit-learn==1.5.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Grid Search pour trouver les meilleurs hyperparamètres qui maximisent la métrique F1\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Pour les métriques financières\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_train_test_data(data= data, start_year = '2010', train_window=5, test_window=1):\n",
    "    df = data.copy()\n",
    "    df.reset_index(inplace=True)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # split data into train and test\n",
    "    train = df[(df['Date'].dt.year >= int(start_year)) & (df['Date'].dt.year < int(start_year) + train_window)]\n",
    "    test = df[(df['Date'].dt.year >= int(start_year) + train_window) & (df['Date'].dt.year < int(start_year) + train_window + test_window)]\n",
    "\n",
    "    X_train = train.drop(columns=['Date', 'stock_target']).values\n",
    "    y_train_return = train['stock_target'].values\n",
    "    # y_train = np.where(y_train > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    X_test = test.drop(columns=['Date', 'stock_target']).values\n",
    "    y_test_return = test['stock_target'].values\n",
    "    # y_test = np.where(y_test > 0, 1, 0) # 0 if stock_target <= 0, 1 otherwise\n",
    "\n",
    "    print(f'X_train from {train[\"Date\"].dt.date.values[0]} to {train[\"Date\"].dt.date.values[-1]}')\n",
    "    print(f'X_test from {test[\"Date\"].dt.date.values[0]} to {test[\"Date\"].dt.date.values[-1]}')\n",
    "    \n",
    "    return X_train, y_train_return, X_test, y_test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_feature_selection(df, alpha=0.01):\n",
    "    data = df.copy()\n",
    "    X = data.drop('stock_target', axis=1)\n",
    "    y = data['stock_target']\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    lasso.fit(X, y)\n",
    "    selected_features = X.columns[lasso.coef_ != 0]\n",
    "    data_selected = data[selected_features.append(pd.Index(['stock_target']))]\n",
    "    print(f'Nombre de features sélectionnées: {len(selected_features)}')\n",
    "    return data_selected\n",
    "\n",
    "def xgboost_grid_search(X_train, y_train, params, num_boost_round=300):\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    recall_scorer = make_scorer(recall_score)\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=params,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "def grid_search_best_params(df, params, model_grid_search):\n",
    "    data = df.copy()\n",
    "    grid_search_params = params.copy()\n",
    "    grid_search_params.pop('nan_strategy')\n",
    "    grid_search_params.pop('lasso_alpha')\n",
    "\n",
    "    best_params = {}\n",
    "    score = 0\n",
    "    for nan_strategy in params['nan_strategy']:\n",
    "        current_params = {'nan_strategy': nan_strategy}\n",
    "        data_lagged = create_lag_variables(data, data.columns)\n",
    "        data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "        data_lagged = data_lagged.dropna()\n",
    "        for lasso_alpha in params['lasso_alpha']:\n",
    "            current_params['lasso_alpha'] = lasso_alpha\n",
    "\n",
    "            # Lasso feature selection\n",
    "            data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "            X = data_selected.drop('stock_target', axis=1)\n",
    "            y = data_selected['stock_target']\n",
    "\n",
    "            # get rolling train test data\n",
    "            X_train, y_train_return, _, _ = get_rolling_train_test_data(\n",
    "                data_selected,\n",
    "                start_year='2010',\n",
    "                train_window=5,\n",
    "                test_window=1)\n",
    "            y_train = np.where(y_train_return > 0, 1, 0)\n",
    "\n",
    "            # grid search\n",
    "            best_params_, best_score_ = model_grid_search(X_train, y_train, grid_search_params)\n",
    "            if best_score_ > score:\n",
    "                best_params = current_params\n",
    "                best_params.update(best_params_)\n",
    "                score = best_score_\n",
    "            \n",
    "    return best_params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.383e+01, tolerance: 5.015e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 221\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 45\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.518e+01, tolerance: 5.015e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 222\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 45\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:10:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 473\n",
      "X_train from 2010-01-28 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:03] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-28 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 473\n",
      "X_train from 2010-01-28 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-28 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:11:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'max_depth': [3, 4],           # Profondeur maximale de l'arbre\n",
    "    'learning_rate': [0.02, 0.01],  # Taux d'apprentissage (eta)\n",
    "    'n_estimators': [100],  # Nombre d'arbres (boost rounds)\n",
    "    'subsample': [0.5, 0.6],     # Fraction des données pour chaque arbre\n",
    "    'colsample_bytree': [0.5]  # Fraction des colonnes pour chaque arbre\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, xgboost_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'interpolate',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'colsample_bytree': 0.5,\n",
       "  'learning_rate': 0.01,\n",
       "  'max_depth': 3,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.5},\n",
       " 0.6060182658055525)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 21\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle avec les meilleurs hyperparamètres\n",
    "data = azn_df.copy()\n",
    "\n",
    "# Créer des variables lags\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=best_params['nan_strategy'])\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "# Feature selection avec Lasso\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=best_params['lasso_alpha'])\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [21:13:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 2010\n",
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree']\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def random_forest_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Vous pouvez également définir d'autres métriques, par exemple :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "\n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,\n",
    "        cv=3,         # Vous pouvez augmenter le nombre de folds (k-fold cross validation)\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.902e-01, tolerance: 5.044e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 103\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 22\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.403e-01, tolerance: 5.044e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 22\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'mean',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'max_depth': 5,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 2,\n",
       "  'n_estimators': 200},\n",
       " 0.5817023273352168)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'n_estimators': [100, 200], # Nombre d'arbres\n",
    "    'max_depth': [None, 5, 10], # Profondeur maximale de l'arbre\n",
    "    'min_samples_split': [2, 5], # Nombre minimum d'échantillons pour diviser un nœud\n",
    "    'min_samples_leaf': [1, 2] # Nombre minimum d'échantillons requis à chaque feuille\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, random_forest_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], min_samples_leaf=best_params['min_samples_leaf'])\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Regression Logistique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "def logistic_regression_grid_search(X_train, y_train, param_grid):\n",
    "    # Définition du modèle\n",
    "    # Note : pour utiliser la pénalisation L1, vous devez choisir un solver qui la supporte\n",
    "    # comme 'liblinear' ou 'saga'.\n",
    "    log_reg_model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "\n",
    "    # Définition de la métrique de scoring\n",
    "    f1_scorer = make_scorer(f1_score, average='binary')\n",
    "    # Exemples d'autres métriques possibles :\n",
    "    # recall_scorer = make_scorer(recall_score, average='binary')\n",
    "    # accuracy_scorer = 'accuracy'\n",
    "    \n",
    "    # Configuration de la recherche en grille\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=log_reg_model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=f1_scorer,  # Vous pouvez mettre 'accuracy', recall_scorer, etc.\n",
    "        cv=3,               # Nombre de folds pour la cross-validation\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Entraînement de GridSearch sur les données\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Récupération des meilleurs paramètres et du meilleur score\n",
    "    return grid_search.best_params_, grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.902e-01, tolerance: 5.044e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 103\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 22\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.403e-01, tolerance: 5.044e-01\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 100\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 22\n",
      "X_train from 2010-01-04 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Nombre de features sélectionnées: 21\n",
      "X_train from 2010-01-19 to 2014-12-31\n",
      "X_test from 2015-01-02 to 2015-12-31\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'nan_strategy': 'mean',\n",
       "  'lasso_alpha': 0.1,\n",
       "  'C': 0.01,\n",
       "  'penalty': 'l1',\n",
       "  'solver': 'liblinear'},\n",
       " 0.5977179187503338)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'nan_strategy': ['mean', 'median', 'interpolate', 'ffill'],  # Stratégie d'imputation\n",
    "    'lasso_alpha': [0.01, 0.1], # Alpha values to explore\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_best_params(data, param_grid, logistic_regression_grid_search)\n",
    "best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n",
      "F1 Score: 0.534\n"
     ]
    }
   ],
   "source": [
    "start_year = '2014'\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year=start_year, train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)\n",
    "\n",
    "# Entraîner le modèle\n",
    "log_reg_model = LogisticRegression(random_state=42, penalty=best_params['penalty'], C=best_params['C'], solver=best_params['solver'], max_iter=10000)\n",
    "\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install tensorflow-macos\n",
    "# !pip install tensorflow-metal\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de features sélectionnées: 98\n",
      "X_train from 2014-01-02 to 2018-12-31\n",
      "X_test from 2019-01-02 to 2019-12-31\n"
     ]
    }
   ],
   "source": [
    "nan_strategy = 'ffill'\n",
    "lasso_alpha = 0.01\n",
    "\n",
    "data_lagged = create_lag_variables(data, data.columns)\n",
    "data_lagged = impute_missing_values(data_lagged, method=nan_strategy)\n",
    "data_lagged = data_lagged.dropna()\n",
    "\n",
    "data_selected = lasso_feature_selection(data_lagged, alpha=lasso_alpha)\n",
    "\n",
    "X = data_selected.drop('stock_target', axis=1)\n",
    "y = data_selected['stock_target']\n",
    "\n",
    "X_train, y_train_return, X_test, y_test_return = get_rolling_train_test_data(data_selected, start_year='2014', train_window=5, test_window=1)\n",
    "y_train = np.where(y_train_return > 0, 1, 0)\n",
    "y_test = np.where(y_test_return > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Créer le modèle\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  \n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # pour classification binaire\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    # metrics accuracy, recall, precision, f1-score\n",
    "    metrics=[keras.metrics.Recall(name='accuracy'), keras.metrics.BinaryAccuracy(name='f1_score')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.5599 - f1_score: 0.4938 - loss: 35.7928 - val_accuracy: 1.0000 - val_f1_score: 0.5476 - val_loss: 21.4896\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6200 - f1_score: 0.5240 - loss: 50.5306 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4524 - val_loss: 12.5517\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4991 - f1_score: 0.4952 - loss: 53.4954 - val_accuracy: 0.8406 - val_f1_score: 0.5516 - val_loss: 3.1648\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.5165 - f1_score: 0.4795 - loss: 54.5681 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4524 - val_loss: 9.0730\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4840 - f1_score: 0.4896 - loss: 53.3517 - val_accuracy: 0.6377 - val_f1_score: 0.5437 - val_loss: 1.7430\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4860 - f1_score: 0.4608 - loss: 50.2937 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4524 - val_loss: 13.5423\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4509 - f1_score: 0.5289 - loss: 43.4985 - val_accuracy: 0.1014 - val_f1_score: 0.4881 - val_loss: 3.0436\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4907 - f1_score: 0.5016 - loss: 44.3333 - val_accuracy: 0.0072 - val_f1_score: 0.4563 - val_loss: 7.0049\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5167 - f1_score: 0.5338 - loss: 38.3738 - val_accuracy: 0.0000e+00 - val_f1_score: 0.4524 - val_loss: 6.6553\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4899 - f1_score: 0.4761 - loss: 38.0908 - val_accuracy: 0.2754 - val_f1_score: 0.5159 - val_loss: 1.4470\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "F1 Score: 0.384\n"
     ]
    }
   ],
   "source": [
    "# Prédire les valeurs sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Print le f1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {round(f1, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
