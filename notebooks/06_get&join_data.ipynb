{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dabereabasse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from stock_data_processor import StockDataProcessor\n",
    "from sentiment_analyzer import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "# processor = StockDataProcessor(start_date, end_date)\n",
    "\n",
    "# aapl_data = processor.get_stock_data('AAPL') # Apple\n",
    "# ebay_data = processor.get_stock_data('EBAY') # eBay\n",
    "# azn_data = processor.get_stock_data('AZN') # AstraZeneca\n",
    "\n",
    "# aapl_data.to_csv('../data/stock_data/AAPL.csv', index=False)\n",
    "# ebay_data.to_csv('../data/stock_data/EBAY.csv', index=False)\n",
    "# azn_data.to_csv('../data/stock_data/AZN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentAnalyzer()\n",
    "\n",
    "# news_data = {\n",
    "#     'AAPL': pd.read_csv('../data/news_data/AAPL.csv'),\n",
    "#     'EBAY': pd.read_csv('../data/news_data/EBAY.csv'),\n",
    "#     'AZN': pd.read_csv('../data/news_data/AZN.csv'),\n",
    "# }\n",
    "\n",
    "# for stock, data in news_data.items():\n",
    "#     print(f'Processing {data.shape[0]} news articles')\n",
    "#     neg, neu, pos = [], [], []\n",
    "#     for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "#         text = row['title'] + ' ' + row['content']\n",
    "#         sentiment = analyzer.analyze_sentiments([text])[0]\n",
    "#         neg.append(sentiment[0])\n",
    "#         neu.append(sentiment[1])\n",
    "#         pos.append(sentiment[2])\n",
    "    \n",
    "#     res = data.copy()\n",
    "#     res['neg'] = neg\n",
    "#     res['neu'] = neu\n",
    "#     res['pos'] = pos\n",
    "\n",
    "#     res.to_csv(f'../data/news_data_with_sentiment/{stock}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit data\n",
    "# from glob import glob\n",
    "# all_paths = glob('../data/reddit_data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit_data_paths = []\n",
    "# for path in all_paths:\n",
    "#     date = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "#     date = pd.to_datetime(date)\n",
    "#     if date >= pd.to_datetime(start_date) and date <= pd.to_datetime(end_date):\n",
    "#         reddit_data_paths.append(path)\n",
    "\n",
    "# reddit_data_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in reddit_data_paths[60:]:\n",
    "#     print(f'Processing {path}')\n",
    "#     date = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "#     df = pd.read_csv(path)\n",
    "\n",
    "#     for s in ['AAPL', 'PHARMA', 'TECH', 'EBAY', 'ECOM']:\n",
    "#         data = df[df['stock/sector'] == s].copy()\n",
    "#         if data.shape[0] == 0: continue\n",
    "\n",
    "#         data[['title', 'selftext']] = data[['title', 'selftext']].fillna('')\n",
    "#         data['created_utc'] = pd.to_datetime(data['created_utc'], unit='s').dt.date\n",
    "#         # sort by created_utc (ascending) and then by score (descending)\n",
    "#         data = data.sort_values(by=['created_utc', 'score'], ascending=[True, False])\n",
    "#         # for each day, keep only the top 10 posts by score\n",
    "#         data = data.groupby('created_utc').head(10)\n",
    "\n",
    "#         neg, neu, pos = [], [], []\n",
    "#         for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "#             text = row['title'] + ' ' + row['selftext']\n",
    "#             sentiment = analyzer.analyze_sentiments([text])[0]\n",
    "#             neg.append(sentiment[0])\n",
    "#             neu.append(sentiment[1])\n",
    "#             pos.append(sentiment[2])\n",
    "\n",
    "#         data['neg'] = neg\n",
    "#         data['neu'] = neu\n",
    "#         data['pos'] = pos\n",
    "\n",
    "#         data.to_csv(f'../data/reddit_data_with_sentiment/{s}/{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # news data\n",
    "# from glob import glob\n",
    "# all_paths = glob('../data/news_data_with_sentiment/*.csv')\n",
    "\n",
    "# for path in all_paths:\n",
    "#     stock = path.split('/')[-1].split('.')[0]\n",
    "#     data = pd.read_csv(path)\n",
    "#     data = data[['date', 'neg', 'neu', 'pos']]\n",
    "#     data.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "#     data = data.groupby('Date').mean().reset_index()\n",
    "#     data.to_csv(f'../data/news_data_with_sentiment_agg/{stock}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reddit data\n",
    "# from glob import glob\n",
    "# stocks = ['AAPL', 'PHARMA', 'TECH', 'EBAY', 'ECOM']\n",
    "\n",
    "# for stock in stocks:\n",
    "#     all_paths = glob(f'../data/reddit_data_with_sentiment/{stock}/*.csv')\n",
    "#     data = pd.DataFrame()\n",
    "#     for path in tqdm(all_paths):\n",
    "#         date = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "#         df = pd.read_csv(path)\n",
    "#         df.rename(columns={'created_utc': 'Date'}, inplace=True)\n",
    "#         df = df[['Date', 'neg', 'neu', 'pos']]\n",
    "#         df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "#         df = df.groupby('Date').mean().reset_index()\n",
    "#         data = pd.concat([data, df], ignore_index=True)\n",
    "#     # sort by date\n",
    "#     data = data.sort_values(by='Date')\n",
    "#     data.to_csv(f'../data/reddit_data_with_sentiment_agg/{stock}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>sp500_return(%)</th>\n",
       "      <th>gold_return(%)</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1.604342</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.311568</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.288173</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  sp500_return(%)  gold_return(%)  vix_close  bond_yields_close\n",
       "0  2010-01-04         1.604342        2.054419  20.040001              0.055\n",
       "1  2010-01-05         0.311568        0.035790  19.350000              0.060\n",
       "2  2010-01-06         0.054552        1.591991  19.160000              0.045\n",
       "3  2010-01-07         0.400120       -0.246505  19.059999              0.045\n",
       "4  2010-01-08         0.288173        0.450091  18.129999              0.040"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# macro data\n",
    "macro_data = pd.read_csv('../data/macro_data/macro_data.csv')\n",
    "macro_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL\n",
      "Processing EBAY\n",
      "Processing AZN\n"
     ]
    }
   ],
   "source": [
    "sectors = {\n",
    "    'AAPL' : 'TECH',\n",
    "    'EBAY' : 'ECOM',\n",
    "    'AZN' : 'PHARMA',\n",
    "}\n",
    "\n",
    "for stock in sectors.keys():\n",
    "    print(f'Processing {stock}')\n",
    "\n",
    "    stock_data = pd.read_csv(f'../data/stock_data_with_indicators/{stock}.csv') # stock data\n",
    "    stock_data.columns = [f'stock_{c}' if c != 'Date' else c for c in stock_data.columns]\n",
    "\n",
    "    news_sentiments = pd.read_csv(f'../data/news_data_with_sentiment_agg/{stock}.csv') # news data\n",
    "    news_sentiments.columns = [f'news_{c}' if c != 'Date' else c for c in news_sentiments.columns]\n",
    "\n",
    "    macro_data = pd.read_csv('../data/macro_data/macro_data.csv') # macro data\n",
    "\n",
    "    if stock != 'AZN':\n",
    "        stock_reddit_sentiments = pd.read_csv(f'../data/reddit_data_with_sentiment_agg/{stock}.csv') # reddit data\n",
    "        stock_reddit_sentiments.columns = [f'stock_reddit_{c}' if c != 'Date' else c for c in stock_reddit_sentiments.columns]\n",
    "\n",
    "    sector_reddit_sentiments = pd.read_csv(f'../data/reddit_data_with_sentiment_agg/{sectors[stock]}.csv') # reddit data\n",
    "    sector_reddit_sentiments.columns = [f'sector_reddit_{c}' if c != 'Date' else c for c in sector_reddit_sentiments.columns]\n",
    "\n",
    "    # merge all data\n",
    "    data = stock_data.merge(news_sentiments, on='Date', how='left')\n",
    "    data = data.merge(macro_data, on='Date', how='left')\n",
    "    if stock != 'AZN':\n",
    "        data = data.merge(stock_reddit_sentiments, on='Date', how='left')\n",
    "    data = data.merge(sector_reddit_sentiments, on='Date', how='left')\n",
    "\n",
    "    data.to_csv(f'../data/merged_data/{stock}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>stock_SMA_10</th>\n",
       "      <th>stock_SMA_15</th>\n",
       "      <th>stock_SMA_20</th>\n",
       "      <th>stock_SMA_50</th>\n",
       "      <th>stock_SMA_100</th>\n",
       "      <th>stock_SMA_200</th>\n",
       "      <th>stock_EMA_10</th>\n",
       "      <th>stock_EMA_12</th>\n",
       "      <th>stock_EMA_14</th>\n",
       "      <th>...</th>\n",
       "      <th>news_neg</th>\n",
       "      <th>news_neu</th>\n",
       "      <th>news_pos</th>\n",
       "      <th>sp500_return(%)</th>\n",
       "      <th>gold_return(%)</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>bond_yields_close</th>\n",
       "      <th>sector_reddit_neg</th>\n",
       "      <th>sector_reddit_neu</th>\n",
       "      <th>sector_reddit_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.978907</td>\n",
       "      <td>0.975040</td>\n",
       "      <td>0.972611</td>\n",
       "      <td>0.963860</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>0.905915</td>\n",
       "      <td>0.982968</td>\n",
       "      <td>0.980982</td>\n",
       "      <td>0.979311</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.604342</td>\n",
       "      <td>2.054419</td>\n",
       "      <td>20.040001</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.239686</td>\n",
       "      <td>0.706169</td>\n",
       "      <td>0.054145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1.001098</td>\n",
       "      <td>0.996297</td>\n",
       "      <td>0.992874</td>\n",
       "      <td>0.983974</td>\n",
       "      <td>0.979251</td>\n",
       "      <td>0.926098</td>\n",
       "      <td>1.002687</td>\n",
       "      <td>1.001063</td>\n",
       "      <td>0.999611</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.311568</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.142433</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.052957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1.010590</td>\n",
       "      <td>1.004731</td>\n",
       "      <td>1.000792</td>\n",
       "      <td>0.992266</td>\n",
       "      <td>0.986986</td>\n",
       "      <td>0.934902</td>\n",
       "      <td>1.008786</td>\n",
       "      <td>1.007701</td>\n",
       "      <td>1.006619</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.054552</td>\n",
       "      <td>1.591991</td>\n",
       "      <td>19.160000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.199394</td>\n",
       "      <td>0.730939</td>\n",
       "      <td>0.069668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1.000837</td>\n",
       "      <td>0.994633</td>\n",
       "      <td>0.990994</td>\n",
       "      <td>0.982151</td>\n",
       "      <td>0.976520</td>\n",
       "      <td>0.926291</td>\n",
       "      <td>0.998328</td>\n",
       "      <td>0.997363</td>\n",
       "      <td>0.996372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026727</td>\n",
       "      <td>0.919697</td>\n",
       "      <td>0.053576</td>\n",
       "      <td>0.400120</td>\n",
       "      <td>-0.246505</td>\n",
       "      <td>19.059999</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.123578</td>\n",
       "      <td>0.822368</td>\n",
       "      <td>0.054053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.998290</td>\n",
       "      <td>0.991278</td>\n",
       "      <td>0.988286</td>\n",
       "      <td>0.978204</td>\n",
       "      <td>0.972674</td>\n",
       "      <td>0.923738</td>\n",
       "      <td>0.995140</td>\n",
       "      <td>0.994161</td>\n",
       "      <td>0.993164</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288173</td>\n",
       "      <td>0.450091</td>\n",
       "      <td>18.129999</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.112057</td>\n",
       "      <td>0.804381</td>\n",
       "      <td>0.083562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  stock_SMA_10  stock_SMA_15  stock_SMA_20  stock_SMA_50  \\\n",
       "0  2010-01-04      0.978907      0.975040      0.972611      0.963860   \n",
       "1  2010-01-05      1.001098      0.996297      0.992874      0.983974   \n",
       "2  2010-01-06      1.010590      1.004731      1.000792      0.992266   \n",
       "3  2010-01-07      1.000837      0.994633      0.990994      0.982151   \n",
       "4  2010-01-08      0.998290      0.991278      0.988286      0.978204   \n",
       "\n",
       "   stock_SMA_100  stock_SMA_200  stock_EMA_10  stock_EMA_12  stock_EMA_14  \\\n",
       "0       0.959357       0.905915      0.982968      0.980982      0.979311   \n",
       "1       0.979251       0.926098      1.002687      1.001063      0.999611   \n",
       "2       0.986986       0.934902      1.008786      1.007701      1.006619   \n",
       "3       0.976520       0.926291      0.998328      0.997363      0.996372   \n",
       "4       0.972674       0.923738      0.995140      0.994161      0.993164   \n",
       "\n",
       "   ...  news_neg  news_neu  news_pos  sp500_return(%)  gold_return(%)  \\\n",
       "0  ...       NaN       NaN       NaN         1.604342        2.054419   \n",
       "1  ...       NaN       NaN       NaN         0.311568        0.035790   \n",
       "2  ...       NaN       NaN       NaN         0.054552        1.591991   \n",
       "3  ...  0.026727  0.919697  0.053576         0.400120       -0.246505   \n",
       "4  ...       NaN       NaN       NaN         0.288173        0.450091   \n",
       "\n",
       "   vix_close  bond_yields_close  sector_reddit_neg  sector_reddit_neu  \\\n",
       "0  20.040001              0.055           0.239686           0.706169   \n",
       "1  19.350000              0.060           0.142433           0.804610   \n",
       "2  19.160000              0.045           0.199394           0.730939   \n",
       "3  19.059999              0.045           0.123578           0.822368   \n",
       "4  18.129999              0.040           0.112057           0.804381   \n",
       "\n",
       "   sector_reddit_pos  \n",
       "0           0.054145  \n",
       "1           0.052957  \n",
       "2           0.069668  \n",
       "3           0.054053  \n",
       "4           0.083562  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
