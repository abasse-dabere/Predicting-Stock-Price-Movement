{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/dabereabasse/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dabereabasse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from stock_data_processor import StockDataProcessor\n",
    "from sentiment_analyzer import SentimentAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2010-01-01'\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "# processor = StockDataProcessor(start_date, end_date)\n",
    "\n",
    "# aapl_data = processor.get_stock_data('AAPL') # Apple\n",
    "# ebay_data = processor.get_stock_data('EBAY') # eBay\n",
    "# azn_data = processor.get_stock_data('AZN') # AstraZeneca\n",
    "\n",
    "# aapl_data.to_csv('../data/stock_data/AAPL.csv', index=False)\n",
    "# ebay_data.to_csv('../data/stock_data/EBAY.csv', index=False)\n",
    "# azn_data.to_csv('../data/stock_data/AZN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzer = SentimentAnalyzer()\n",
    "\n",
    "# news_data = {\n",
    "#     'AAPL': pd.read_csv('../data/news_data/AAPL.csv'),\n",
    "#     'EBAY': pd.read_csv('../data/news_data/EBAY.csv'),\n",
    "#     'AZN': pd.read_csv('../data/news_data/AZN.csv'),\n",
    "# }\n",
    "\n",
    "# for stock, data in news_data.items():\n",
    "#     print(f'Processing {data.shape[0]} news articles')\n",
    "#     neg, neu, pos = [], [], []\n",
    "#     for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "#         text = row['title'] + ' ' + row['content']\n",
    "#         sentiment = analyzer.analyze_sentiments([text])[0]\n",
    "#         neg.append(sentiment[0])\n",
    "#         neu.append(sentiment[1])\n",
    "#         pos.append(sentiment[2])\n",
    "    \n",
    "#     res = data.copy()\n",
    "#     res['neg'] = neg\n",
    "#     res['neu'] = neu\n",
    "#     res['pos'] = pos\n",
    "\n",
    "#     res.to_csv(f'../data/news_data_with_sentiment/{stock}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit data\n",
    "from glob import glob\n",
    "all_paths = glob('../data/reddit_data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_paths = []\n",
    "for path in all_paths:\n",
    "    date = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    date = pd.to_datetime(date)\n",
    "    if date >= pd.to_datetime(start_date) and date <= pd.to_datetime(end_date):\n",
    "        reddit_data_paths.append(path)\n",
    "\n",
    "reddit_data_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/reddit_data/RS_2015-01.csv',\n",
       " '../data/reddit_data/RS_2015-02.csv',\n",
       " '../data/reddit_data/RS_2015-03.csv',\n",
       " '../data/reddit_data/RS_2015-04.csv',\n",
       " '../data/reddit_data/RS_2015-05.csv',\n",
       " '../data/reddit_data/RS_2015-06.csv',\n",
       " '../data/reddit_data/RS_2015-07.csv',\n",
       " '../data/reddit_data/RS_2015-08.csv',\n",
       " '../data/reddit_data/RS_2015-09.csv',\n",
       " '../data/reddit_data/RS_2015-10.csv',\n",
       " '../data/reddit_data/RS_2015-11.csv',\n",
       " '../data/reddit_data/RS_2015-12.csv',\n",
       " '../data/reddit_data/RS_2016-01.csv',\n",
       " '../data/reddit_data/RS_2016-02.csv',\n",
       " '../data/reddit_data/RS_2016-03.csv',\n",
       " '../data/reddit_data/RS_2016-04.csv',\n",
       " '../data/reddit_data/RS_2016-05.csv',\n",
       " '../data/reddit_data/RS_2016-06.csv',\n",
       " '../data/reddit_data/RS_2016-07.csv',\n",
       " '../data/reddit_data/RS_2016-08.csv',\n",
       " '../data/reddit_data/RS_2016-09.csv',\n",
       " '../data/reddit_data/RS_2016-10.csv',\n",
       " '../data/reddit_data/RS_2016-11.csv',\n",
       " '../data/reddit_data/RS_2016-12.csv',\n",
       " '../data/reddit_data/RS_2017-01.csv',\n",
       " '../data/reddit_data/RS_2017-02.csv',\n",
       " '../data/reddit_data/RS_2017-03.csv',\n",
       " '../data/reddit_data/RS_2017-04.csv',\n",
       " '../data/reddit_data/RS_2017-05.csv',\n",
       " '../data/reddit_data/RS_2017-06.csv',\n",
       " '../data/reddit_data/RS_2017-07.csv',\n",
       " '../data/reddit_data/RS_2017-08.csv',\n",
       " '../data/reddit_data/RS_2017-09.csv',\n",
       " '../data/reddit_data/RS_2017-10.csv',\n",
       " '../data/reddit_data/RS_2017-11.csv',\n",
       " '../data/reddit_data/RS_2017-12.csv',\n",
       " '../data/reddit_data/RS_2018-01.csv',\n",
       " '../data/reddit_data/RS_2018-02.csv',\n",
       " '../data/reddit_data/RS_2018-03.csv',\n",
       " '../data/reddit_data/RS_2018-04.csv',\n",
       " '../data/reddit_data/RS_2018-05.csv',\n",
       " '../data/reddit_data/RS_2018-06.csv',\n",
       " '../data/reddit_data/RS_2018-07.csv',\n",
       " '../data/reddit_data/RS_2018-08.csv',\n",
       " '../data/reddit_data/RS_2018-09.csv',\n",
       " '../data/reddit_data/RS_2018-10.csv',\n",
       " '../data/reddit_data/RS_2018-11.csv',\n",
       " '../data/reddit_data/RS_2018-12.csv',\n",
       " '../data/reddit_data/RS_2019-01.csv',\n",
       " '../data/reddit_data/RS_2019-02.csv',\n",
       " '../data/reddit_data/RS_2019-03.csv',\n",
       " '../data/reddit_data/RS_2019-04.csv',\n",
       " '../data/reddit_data/RS_2019-05.csv',\n",
       " '../data/reddit_data/RS_2019-06.csv',\n",
       " '../data/reddit_data/RS_2019-07.csv',\n",
       " '../data/reddit_data/RS_2019-08.csv',\n",
       " '../data/reddit_data/RS_2019-09.csv',\n",
       " '../data/reddit_data/RS_2019-10.csv',\n",
       " '../data/reddit_data/RS_2019-11.csv',\n",
       " '../data/reddit_data/RS_2019-12.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_data_paths[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/reddit_data/RS_2015-01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 582/8325 [00:48<11:11, 11.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2842 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 8325/8325 [12:10<00:00, 11.40it/s]\n",
      "100%|██████████| 6452/6452 [08:23<00:00, 12.82it/s]\n",
      "100%|██████████| 15978/15978 [16:15<00:00, 16.38it/s] \n",
      "100%|██████████| 661/661 [01:01<00:00, 10.75it/s]\n",
      "100%|██████████| 333/333 [00:29<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/reddit_data/RS_2015-02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7406/7406 [10:01<00:00, 12.31it/s]\n",
      "100%|██████████| 6690/6690 [07:16<00:00, 15.31it/s]\n",
      "100%|██████████| 15037/15037 [15:22<00:00, 16.30it/s]\n",
      "100%|██████████| 664/664 [00:59<00:00, 11.24it/s]\n",
      "100%|██████████| 348/348 [00:36<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/reddit_data/RS_2015-03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10816/10816 [14:20<00:00, 12.57it/s]\n",
      "100%|██████████| 7019/7019 [07:34<00:00, 15.44it/s]\n",
      "100%|██████████| 16197/16197 [16:25<00:00, 16.44it/s]\n",
      "100%|██████████| 656/656 [00:57<00:00, 11.33it/s]\n",
      "100%|██████████| 389/389 [00:37<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/reddit_data/RS_2015-04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 3851/16025 [04:36<14:33, 13.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(data\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     14\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselftext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m     sentiment \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_sentiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m     neg\u001b[38;5;241m.\u001b[39mappend(sentiment[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m     neu\u001b[38;5;241m.\u001b[39mappend(sentiment[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/GitHub/Predicting-Stock-Price-Movement/notebooks/../scripts/sentiment_analyzer.py:84\u001b[0m, in \u001b[0;36mSentimentAnalyzer.analyze_sentiments\u001b[0;34m(self, texts, mode)\u001b[0m\n\u001b[1;32m     81\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 84\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_sentiment_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     neg, neut, pos \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m], scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m], scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msep\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path in reddit_data_paths[60:]:\n",
    "    print(f'Processing {path}')\n",
    "    date = path.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for s in ['AAPL', 'PHARMA', 'TECH', 'EBAY', 'ECOM']:\n",
    "        data = df[df['stock/sector'] == s].copy()\n",
    "        if data.shape[0] == 0: continue\n",
    "\n",
    "        data[['title', 'selftext']] = data[['title', 'selftext']].fillna('')\n",
    "\n",
    "        neg, neu, pos = [], [], []\n",
    "        for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            text = row['title'] + ' ' + row['selftext']\n",
    "            sentiment = analyzer.analyze_sentiments([text])[0]\n",
    "            neg.append(sentiment[0])\n",
    "            neu.append(sentiment[1])\n",
    "            pos.append(sentiment[2])\n",
    "\n",
    "        data['neg'] = neg\n",
    "        data['neu'] = neu\n",
    "        data['pos'] = pos\n",
    "\n",
    "        data.to_csv(f'../data/reddit_data_with_sentiment/{s}/{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
